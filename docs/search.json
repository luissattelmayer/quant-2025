[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Advanced RStudio Labsessions",
    "section": "",
    "text": "Course Overview\nThis repository contains all the course material for the RStudio Labsessions for the Spring semester 2025 at the School of Research at SciencesPo Paris. The class follows Brenda van Coppenolle’s and Jan Rovny’s lecture on Quantitative Methods II. Furthermore, the RStudio part of the course is a direct continuation of Malo Jan’s RStudio introduction course. If you feel the need to go back to some basics of general R use, data management or visualization, feel free to check out his course’s website. Rest assured, however, that 1) we will recap plenty of things, 2) make slow but steady progress, 3) and come back to the essentials of data wrangling again during the semester while building statistical models.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Structure",
    "text": "Course Structure\nIn total we will see each other 6 times. The lessons will be structured in such a way that I will first present something to you and explain my script. Ideally, you will then start coding in groups of 2 and work on exercises related to the topic. You can find more information about the exercises in the subsection “course validation”. I will of course be there to help you. The rest you solve at home and send me your final script. At the beginning of each next meeting we will go through the solutions together. Also, I upload my own script before each session, so you can use it as a template when solving the tasks and also later, when the course is over, as a template for further coding (if you like of course…).\n\n\n\nSession\nDescription\nDates\n\n\n\n\nSession 1\nRStudio Recap & OLS\n29/01 & 05/02\n\n\nSession 2\nLogistic Regressions\n12/02 & 19/02\n\n\nSession 3\nMultinomial Regression\n05/03 & 12/03\n\n\nSession 4\nCausal Inference I\n19/03 & 26/03\n\n\nSession 5\nCausal Inference II\n02/04 & 09/04\n\n\nSession 6\nTime Series\n16/04 & 23/04",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#course-validation",
    "href": "index.html#course-validation",
    "title": "Advanced RStudio Labsessions",
    "section": "Course Validation",
    "text": "Course Validation\nIn the two weeks between each lecture, you will be given exercises to upload to the designated link for each session. The document where you write the solutions must be written in Markdown format.\nI will grade your solutions to my exercises on a 0 to 5 scale. I would like to see that you have done something and hopefully finished the exercise. If you are unable to finish the exercise, it is no problem and I do understand that not everybody feels as comfortable with R as some other people might do. Handing something in is key to getting points! This class can be finished by everyone and I do not want you to worry about your grade too much. But I would like that you all at least try to solve the exercises! Work in groups of two and try to hand in something after each session. The precise deadline will be communicated in class, the course’s GitHub page and on the Moodle page.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#optional-course-parts",
    "href": "index.html#optional-course-parts",
    "title": "Advanced RStudio Labsessions",
    "section": "Optional Course Parts",
    "text": "Optional Course Parts\nWhen I taught the course last year, some students approached me and asked for several levels of difficulty. I will try to implement this in the homework and in class. I have also decided to add an optional part to each session. In the optional parts, I will introduce new packages, advanced methods, and I will also upload a few scripts in the appendix on things like text-as-data, webscraping or similar, if I have the time. Also – again, if the times allows it – I will go through these optional parts in class. But please be assured that if you decide not to follow the optional parts, that is okay. But if you do, I can promise you will make better and faster progress. Lastly, if you are interested in certain things, want to learn about specific methods or how to implement things or workflows in RStudio, please do not hesitate to contact me and I will see if I can squeeze it in somewhere.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Advanced RStudio Labsessions",
    "section": "Requirements",
    "text": "Requirements\nYou must have downloaded R and RStudio by the beginning of the course (you need to install both!) before our sessions. Please let me know if you encounter any problems during the installation. Here is a quick guide on how to do that: https://rstudio-education.github.io/hopr/starting.html\nR and RStudio are both free and open source. You need both of them installed in order to operate with the R coding language.\nFor R, go on the CRAN website and download the file for your respective operating system: https://cran.r-project.org/ For RStudio, you need to do the same thing by clicking on this link: https://posit.co/products/open-source/rstudio/ RStudio has received a new name recently (“posit”) but you will still find all the necessary steps behind this link under the name of RStudio.\nOtherwise, there are few prerequisites except that you must bring your computer to the sessions with the required programs installed. I will provide you with datasets in each case and I will explain everything else in the course.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#help-and-office-hours",
    "href": "index.html#help-and-office-hours",
    "title": "Advanced RStudio Labsessions",
    "section": "Help and Office Hours",
    "text": "Help and Office Hours\nThere are unfortunately no regular office hours. But please do not hesitate to reach out, if you have any concerns, questions or feedback for me! My inbox is always open. I tend to reply quickly but in the case that I have not replied in under 48h, simply send the email again. I will not be offended!\nLearning how to code and working with RStudio can be a struggle and a tough task. I have started out once like you and I will try to keep that in mind. Feel free to always ask questions in class or if you see me on campus. The most important thing, however, is that you try!",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Advanced RStudio Labsessions",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis is only a quick section to give credit, where credit is due! Malo Jan is one of my daily inspirations for anything that has to do with research and RStudio. I have taught this class already last year and had most of my scripts written in PDFs but Rohan Alexander’s book Telling Stories with Data served as a new inspiration to write this course in its Quarto book format. Also shout outs to Felix Lennert and some of his ideas for the homework.",
    "crumbs": [
      "Introduction",
      "Course Overview"
    ]
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "1.1 Introduction\nThis is a short recap of things you have seen last year and will need this year as well. It will refresh your understanding of the linear regression method called ordinary least squares (OLS). This script is supposed to serve as a cheat sheet for you to which you can always come back to.\nThese are the main points of today’s session and script:\nThese are the packages we will be using in this session:\nneeds(\n  tidyverse,\n  rio, \n  stargazer,\n  broom,\n)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#introduction",
    "href": "session1/session1.html#introduction",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "A refresher of Ordinary Least Squares (OLS)\nWhat is Base R and what are packages?\nA recap of basic coding in R\nBuilding & Interpreting a simple linear model\nVisualizing Residuals\nThe broom package (OPTIONAL!)",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#ordinary-least-squares-ols",
    "href": "session1/session1.html#ordinary-least-squares-ols",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.2 Ordinary Least Squares (OLS)",
    "text": "1.2 Ordinary Least Squares (OLS)\nOLS regressions are the powerhouse of statistics. The world must have been a dark place without them. They are the most basic form of linear regression and are used to predict the value of a dependent variable (DV) based on the value of independent variables (IVs). It is important to note that the relationship between the DV and the IVs is assumed to be linear.\nAs a quick reminder, this is the formula for a basic linear model: \\(\\widehat{Y} = \\widehat{\\alpha} + \\widehat{\\beta} X\\).\nOLS is a certain kind of method of linear model in which we choose the line which has the least prediction errors. This means that it is the best way to fit a line through all the residuals with the least errors. It minimizes the sum of the squared prediction errors \\(\\text{SSE} = \\sum_{i=1}^{n} \\widehat{\\epsilon}_i^2\\)\nFive main assumptions have to be met to allow us to construct an OLS model:\n\nLinearity: Linear relationship between IVs and DVs\nNo endogeneity between \\(y\\) and \\(x\\)\n\nErrors are normally distributed\nHomoscedasticity (variance of errors is constant)\nNo multicolinearity (no linear relationship between the independent variables)\n\nFor this example, I will be working with some test scores of a midterm and a final exam which I once had to work through. We are trying to see if there is a relationship between the score in the midterm and the grade of the final exam. Theoretically speaking, we would expect most of the students who did well on the first exam to also get a decent grade on the second exam. If our model indicates a statistical significance between the independent and the dependent variable and a positive coefficient of the former on the latter, this theoretical idea then holds true.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#coding-recap",
    "href": "session1/session1.html#coding-recap",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.3 Coding Recap",
    "text": "1.3 Coding Recap\nBefore we start, let’s refresh our coding basics again. RStudio works with packages and libraries. There is something called Base R, which is the basic infrastructure that R always comes with when you install it. The R coding language has a vibrant community of contributors who have written their own packages and libraries which you can install and use. As Malo, I am of the tidyverse school and mostly code with this package or in its style when I am wrangling with data, changing its format or values and so on. Here and there, I will, however, try to provide you with code that uses Base R or other packages. In coding, there are many ways to achieve the same goal – and I will probably be repeating this throughout the semester – and we always strive for the fastest or most automated way. I will not force the tidyverse environment on you but I do think that it is one of the most elegant and fastest way of doing statistics in R. It is sort of my RStudio dialect but you can obviously stick to yours if you have found it. Also, as long as you find a way that works for you, that is fine with me!\nTo load the packages, we are going to need:\n\nlibrary(tidyverse)\n\nNext we will import the dataset of grades.\n\ndata &lt;- read_csv(\"course_grades.csv\")\n\nRows: 200 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): midterm|final_exam|final_grade|var1|var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe path which I specify in the read_csv file is short as this quarto document has the same working directory to which the data set is also saved. If you, for example, have your dataset on your computer’s desktop, you can access it via some code like this one:\n\ndata &lt;- read_csv(\"~/Desktop/course_grades.csv\")\n\nOr if it is within a folder on your desktop:\n\ndata &lt;- read_csv(\"~/Desktop/folder/course_grades.csv\")\n\n\n\n\n\n\n\nI will be only working within .Rproj files and so should you. 1 This is the only way to ensure that your working directory is always the same and that you do not have to change the path to your data set every time you open a new RStudio session. Further, this is the only way to make sure that other collaborators can easily open your project and work with it as well. Simply zip the file folder in which you have your code and\n\n\n\nYou can also import a dataset directly from the internet. Several ways are possible that all lead to the same end result:\n\ndataset_from_internet_1 &lt;- read_csv(\"https://www.chesdata.eu/s/1999-2019_CHES_dataset_meansv3.csv\")\n  \n# this method uses the rio package\nlibrary(rio)\ndataset_from_internet_2 &lt;- import(\"https://jan-rovny.squarespace.com/s/ESS_FR.dta\")\n\nLet’s take a first look at the data which we just imported:\n\n# tidyverse\nglimpse(data)\n\nRows: 200\nColumns: 1\n$ `midterm|final_exam|final_grade|var1|var2` &lt;chr&gt; \"17.4990613754243|15.641013…\n\n# Base R\nstr(data)\n\nspc_tbl_ [200 × 1] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ midterm|final_exam|final_grade|var1|var2: chr [1:200] \"17.4990613754243|15.64101334897|17.63|NA|NA\" \"17.7446326301825|18.7744366510731|14.14|NA|NA\" \"13.9316618079058|14.9978584022336|18.2|NA|NA\" \"10.7068243984724|11.9479428399047|19.85|NA|NA\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `midterm|final_exam|final_grade|var1|var2` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nSomething does not look right, this happens quite frequently when saving a csv file. It stands for comma separated value. R is having trouble reading this file since I have saved all grades with commas instead of points. Thus, we need to use the read_delim() function. Sometimes the read_csv2() function also does the trick. You’d be surprised by how often you encounter this problem. This is simply to raise your awareness to it!\nThe read_delim() function is the overall function of the readr package to read any sort of data file, whereas read_csv() and read_csv2() are specific functions to read csv files. The read_delim() function has a delim argument which you can use to specify the delimiter of your data file. For the sake of the example, I had purposefully saved the csv file using the | delimiter.\n\ndata &lt;- read_delim(\"course_grades.csv\", delim = \"|\")\n\nRows: 200 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"|\"\ndbl (3): midterm, final_exam, final_grade\nlgl (2): var1, var2\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(data)\n\nRows: 200\nColumns: 5\n$ midterm     &lt;dbl&gt; 17.499061, 17.744633, 13.931662, 10.706824, 17.118799, 17.…\n$ final_exam  &lt;dbl&gt; 15.641013, 18.774437, 14.997858, 11.947943, 15.694728, 17.…\n$ final_grade &lt;dbl&gt; 17.63, 14.14, 18.20, 19.85, 14.67, 20.26, 16.90, 13.40, 12…\n$ var1        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ var2        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\n\nThis time, it has been properly imported. But by looking closer at it, we can see that there are two columns in the data frame that are empty and do not even have a name. We need to get rid of these first. Here are several ways of doing this. In coding, many ways lead to the same goal. In R, some come with a specific package, some use Base R. It is up to you to develop your way of doing things.\n\n# This is how you could do it in Base R\ndata &lt;- data[, -c(4, 5)]\n\n# Using the select() function of the dplyr package you can drop the fourth\n# and fifth columns by their position using the - operator and the -c() to\n# remove multiple columns\ndata &lt;- data  |&gt;  select(-c(4, 5))\n\n# I have stored the mutated data set in the old object; \n# you can also just transform the object itself...\ndata |&gt; select(-c(4, 5))\n\n# ... or create a new one\ndata_2 &lt;- data |&gt; select(-c(4, 5))\n\nNow that we have set up our data frame, we can build our OLS model. For that, we can simply use the lm() function that comes with Base R, it is built into R so to speak. In this function, we specify the data and then construct the model by using the tilde (~) between the dependent variable and the independent variable(s). Store your model in an object which can later be subject to further treatment and analysis.\n\nmodel &lt;- lm(final_exam ~ midterm, data)\nsummary(model)\n\n\nCall:\nlm(formula = final_exam ~ midterm, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6092 -0.8411 -0.0585  0.8712  3.3086 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.62482    0.73212   6.317 1.72e-09 ***\nmidterm      0.69027    0.04819  14.325  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.34 on 198 degrees of freedom\nMultiple R-squared:  0.5089,    Adjusted R-squared:  0.5064 \nF-statistic: 205.2 on 1 and 198 DF,  p-value: &lt; 2.2e-16\n\n\nSince the summary() function only shows us something in our console and the output is not very pretty, I encourage you to use the broom package for a nicer regression table.\n\nbroom::tidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nYou can also use the stargazer package in order to export your tables to text or LaTeX format which you can then copy to your documents.\n\nlibrary(stargazer)\nstargazer(model, type = \"text\", out = \"latex\")\n\n\n===============================================\n                        Dependent variable:    \n                    ---------------------------\n                            final_exam         \n-----------------------------------------------\nmidterm                      0.690***          \n                              (0.048)          \n                                               \nConstant                     4.625***          \n                              (0.732)          \n                                               \n-----------------------------------------------\nObservations                    200            \nR2                             0.509           \nAdjusted R2                    0.506           \nResidual Std. Error      1.340 (df = 198)      \nF Statistic          205.196*** (df = 1; 198)  \n===============================================\nNote:               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#interpretation-of-ols-results",
    "href": "session1/session1.html#interpretation-of-ols-results",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.4 Interpretation of OLS Results",
    "text": "1.4 Interpretation of OLS Results\nHow do we interpret this?\n\n\nR2: Imagine you’re trying to draw a line that best fits a bunch of dots (data points) on a graph. The R-squared value is a way to measure how well that line fits the dots. It’s a number between 0 and 1, where 0 means the line doesn’t fit the dots at all and 1 means the line fits the dots perfectly. R-squared tells us how much of the variation in the dependent variable is explained by the variation in the predictor variables.\n\nAdjusted R2: Adjusted R-squared is the same thing as R-squared, but it adjusts for how many predictor variables you have. It’s like a better indicator of how well the line fits the dots compared to how many dots you’re trying to fit the line to. It always adjusts the R-squared value to be a bit lower so you always want your adjusted R-squared value to be as high as possible.\n\nResidual Std. Error: The residual standard error is a way to measure the average distance between the line you’ve drawn (your model’s predictions) and the actual data points. It’s like measuring how far off the line is from the actual dots on the graph. Another way to think about this is like a test where you want to get as many answers correct as possible and if you are off by a lot in your answers, the residual standard error would be high, but if you are only off by a little, the residual standard error would be low. So in summary, lower residual standard error is better, as it means that the model is making predictions that are closer to the true values in the data.\n\nF Statistics: The F-statistic is like a test score that tells you how well your model is doing compared to a really simple model. It’s a way to check if the model you’ve built is any better than just guessing. A large F-statistic means that your model is doing much better than just guessing.",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#visualizing-the-regression-line",
    "href": "session1/session1.html#visualizing-the-regression-line",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.5 Visualizing the Regression Line",
    "text": "1.5 Visualizing the Regression Line\nJust for fun and to refresh you ggplot knowledge, let’s visualize the regression line. Here, we specify\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n    labs(\n    title = \"Relationship between Midterm and Final Exam Scores\",\n    x = \"Midterm Scores\",\n    y = \"Final Exam Scores\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nRemember how I told you above that OLS was about finding the smallest amount of squared errors! This is what we can visualize here. The red lines are the distance from the residuals to the fitted line. The OLS line is the line that minimizes the sum of the squared errors!\n\nggplot(data, aes(x = midterm, y = final_exam)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  # highlight the OLS logic graphically\n  geom_segment(\n    aes(\n      x = midterm,\n      y = final_exam,\n      xend = midterm,\n      yend = predict(model),\n      color = \"red\",\n      alpha = 0.5,\n    )) +\n  labs(title = \"Relationship between Midterm and Final Exam Scores\",\n       x = \"Midterm Scores\",\n       y = \"Final Exam Scores\") +\n  guides(color = FALSE, alpha = FALSE) +\n  theme_minimal()\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#the-broom-package-optional",
    "href": "session1/session1.html#the-broom-package-optional",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "\n1.6 The broom package (OPTIONAL!)",
    "text": "1.6 The broom package (OPTIONAL!)\nThe broom package in R is designed to bridge the gap between R’s statistical output and tidy data. 2 It takes the output of various R statistical functions and turns them into tidy data frames. This is particularly useful because many of R’s modeling functions return outputs that are not immediately suitable for further data analysis or visualization within the tidyverse framework.\n\n\n\n\n\n1.6.1 Nesting with nest()\n\nNesting in the context of the broom package usually refers to the idea of creating a list-column in a data frame (or even better a tibble) where each element of this list-column is itself a data frame (again, even better a tibble) or a model object. In a complex analysis, you might fit separate models to different subsets of data. Nesting allows you to store each of these models (or their broom-tidied summaries that we will see in the next three sub-sections) within a single, larger data frame (for the third time, the best is to do this with a tibble) for easier manipulation and analysis. For questions on what tibbles are see the below section on tibbles.\nLet’s go back to the midterm data which I have used before and randomly assign students into two classes (Class A or Class B) pretending that two different classes of students had taken the exams. I will then later on nest the data by class so that you can understand the logic of nesting.\n\ndata &lt;- data |&gt; \n  mutate(class = sample(c(\"A\", \"B\"), size = nrow(data), replace = TRUE))\n\nNow I will group my observations by class using group_by() and then nest them within these groups. The output will only contain two rows, class A and class B, and each row will contain a tibble with the observations of the respective class.\n\ndata |&gt; \n  group_by(class) |&gt;\n  nest()\n\n# A tibble: 2 × 2\n# Groups:   class [2]\n  class data              \n  &lt;chr&gt; &lt;list&gt;            \n1 A     &lt;tibble [105 × 3]&gt;\n2 B     &lt;tibble [95 × 3]&gt; \n\n\nThat is a very simple application of a nesting process. You can group_by() and nest() by many different variables. You might want to nest your observations per year or within countries. You can also nest by multiple variables at the same time. We will see this idea again in the next session when we will talk about the purrr package and how to automatically run regressions for several countries at the same time\n\n1.6.2 Model estimates with tidy()\n\nThe tidy() function takes statistical output ofa model and turns it into a tidy tibble. This means each row is an observation (e.g., a coefficient in a regression output) and each column is a variable (e.g., estimate, standard error, statistic). For instance, after fitting a linear model, you can use tidy() to create a data frame where each row represents a coefficient, with columns for estimates, standard errors, t-values, and p-values.\n\n1.6.3 Key metrics with glance()\n\nglance() provides a one-row summary of a model’s information. It captures key metrics that describe the overall quality or performance of a model, like \\(R^2\\), AIC, BIC in the context of linear models. This is useful for getting a quick overview of a model’s performance metrics.\n\n1.6.4 Residuals with augment()\n\nThe augment() function adds information about individual observations to the original data, such as fitted values or residuals in a regression model. You want to do this when you are evaluating a model fit at the observation level, checking for outliers, or understanding the influence of individual data points. We will talk more about model diagnostics in the next session!\nLet’s take a look at how this would work out in practice. For simplicity sake, we will set the nest() logic aside for a second and only look at the tidy(), glance(), and augment() functions.\nHere I only build the same model that we have already seen above:\n\nmodel &lt;- lm(final_exam ~ midterm, data = data)\n\n\ntidy(model)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    4.62     0.732       6.32 1.72e- 9\n2 midterm        0.690    0.0482     14.3  2.10e-32\n\n\nNow let’s glance the hell out of the model:\n\nglance(model)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.509         0.506  1.34      205. 2.10e-32     1  -341.  689.  699.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nDon’t worry about what these things might mean for now, AIC and BIC will for example come up again next session.\n\n1.6.5 What is a tibble?",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session1/session1.html#footnotes",
    "href": "session1/session1.html#footnotes",
    "title": "\n1  RStudio Recap & OLS\n",
    "section": "",
    "text": "Malo’s explanation and way of introducing you to RStudio projects can be found here.↩︎\nAs a quick reminder these three principles are guiding when we speak of tidy data: 1) Every column is a variable, 2) Every row is an observation, 3) Every cell is a single value.↩︎",
    "crumbs": [
      "OLS Recap",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>RStudio Recap & OLS</span>"
    ]
  },
  {
    "objectID": "session2/session2.html",
    "href": "session2/session2.html",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "2.1 Introduction\nYou have seen the logic of Logistic Regressions with Professor Rovny in the lecture. In this lab session, we will understand how to apply this logic to R and how to build a model, interpret and visualize its results and how to run some diagnostics on your models. If the time allows it, I will also show you how automatize the construction of your model and run several logistic regressions for many countries at once.\nThese are the main points of today’s session and script:",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#introduction",
    "href": "session2/session2.html#introduction",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "Getting used to the European Social Survey\nCleaning data: dropping rows, columns, creating and mutating variables\nBuilding a generalized linear model (glm()); special focus on logit/probit\nExtracting and interpreting the coefficients\nVisualization of results\nIntroduction to the purrr package and automation in RStudio (OPTIONAL!)",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#data-management-data-cleaning",
    "href": "session2/session2.html#data-management-data-cleaning",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.2 Data Management & Data Cleaning",
    "text": "2.2 Data Management & Data Cleaning\nAs I have mentioned last session, I will try to gradually increase the data cleaning part. It is integral to R and operationalizing our quantitative questions in models. A properly cleaned data set is worth a lot. This time we will work on how to drop values of variables (and thus rows of our dataset) which we are either not interested in or, most importantly, because they skew our estimations.\n\n# these are the packages, I will need for this session \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#importing-the-data",
    "href": "session2/session2.html#importing-the-data",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.3 Importing the data",
    "text": "2.3 Importing the data\nWe have seen how to import a dataset. Create an .Rproj of your choice and create a folder in which the dataset of this lecture resides. You can download this dataset from our Moodle page. I have pre-cleaned it a bit. If you were to download this wave of the European Social Survey from the Internet, it would be a much bigger data set. I encourage you to do this and try to figure out ways to manipulate your data but for now, we’ll stick to the slightly cleaner version.\n\n# importing the data; if you are unfamiliar with this operator |&gt; , ask me or\n# go to my document \"Recap of RStudio\" which you can find on Moodle\ness &lt;- read_csv(\"ESS_10_fr.csv\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAs you can see from the dataset’s name, we are going to work with the European Social Survey. It is the biggest, most comprehensive and perhaps also most important survey on social and political life in the European Union. It comes in waves of two years and all the European states which want to pay for it produce their own data. In fact, the French surveys (of which we are going to use the most recent, 10th wave) are produced at SciencesPo, at the Centre de Données Socio-Politiques (CDSP)!\nThe ESS is extremely versatile if you need a broad and comprehensive data set for both national politics in Europe or to compare European countries. Learning how to use it, how to manage and clean the ESS waves will give you all the instruments to work with almost any data set that is “out there”. Also, some of you might want to use the ESS waves for your theses or research papers. There is a lot that can be done with it, not only cross-sectionally but also over time. So give it a try :)\nEnough advertisement for the ESS, let’s get back to wrangling with our data! As always, the first step is to inspect (“glimpse”) at our data and the data frame’s structure. We do this to see if obvious issues arise at a first glance.\n\nglimpse(ess)\n\nRows: 33,351\nColumns: 25\n$ name     &lt;chr&gt; \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS10e02_2\", \"ESS1…\n$ essround &lt;dbl&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ edition  &lt;dbl&gt; 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2.2, 2…\n$ proddate &lt;chr&gt; \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.12.2022\", \"21.1…\n$ idno     &lt;dbl&gt; 10002, 10006, 10009, 10024, 10027, 10048, 10053, 10055, 10059…\n$ cntry    &lt;chr&gt; \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"BG\", \"…\n$ dweight  &lt;dbl&gt; 1.9393836, 1.6515952, 0.3150246, 0.6730366, 0.3949991, 0.8889…\n$ pspwght  &lt;dbl&gt; 1.2907065, 1.4308782, 0.1131722, 1.4363747, 0.5848892, 0.6274…\n$ pweight  &lt;dbl&gt; 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177165, 0.2177…\n$ anweight &lt;dbl&gt; 0.28100810, 0.31152576, 0.02463945, 0.31272244, 0.12734002, 0…\n$ prob     &lt;dbl&gt; 0.0003137546, 0.0003684259, 0.0019315645, 0.0009040971, 0.001…\n$ stratum  &lt;dbl&gt; 185, 186, 175, 148, 138, 182, 157, 168, 156, 135, 162, 168, 1…\n$ psu      &lt;dbl&gt; 2429, 2387, 2256, 2105, 2065, 2377, 2169, 2219, 2155, 2053, 2…\n$ polintr  &lt;dbl&gt; 4, 1, 3, 4, 1, 1, 3, 3, 3, 3, 1, 4, 2, 2, 3, 3, 2, 2, 4, 2, 3…\n$ trstplt  &lt;dbl&gt; 3, 6, 3, 0, 0, 0, 5, 1, 2, 0, 5, 4, 7, 5, 2, 2, 2, 2, 0, 3, 0…\n$ trstprt  &lt;dbl&gt; 3, 7, 2, 0, 0, 0, 3, 1, 2, 0, 7, 4, 2, 6, 2, 1, 3, 1, 0, 3, 3…\n$ vote     &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2…\n$ prtvtefr &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ clsprty  &lt;dbl&gt; 2, 1, 1, 2, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2…\n$ gndr     &lt;dbl&gt; 2, 1, 2, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1…\n$ yrbrn    &lt;dbl&gt; 1945, 1978, 1971, 1970, 1951, 1990, 1981, 1973, 1950, 1950, 1…\n$ eduyrs   &lt;dbl&gt; 12, 16, 16, 11, 17, 12, 12, 12, 11, 3, 12, 12, 15, 15, 19, 11…\n$ emplrel  &lt;dbl&gt; 1, 3, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1…\n$ uemp12m  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n$ uemp5yr  &lt;dbl&gt; 6, 2, 1, 6, 6, 6, 1, 6, 6, 6, 6, 6, 6, 6, 6, 2, 6, 6, 6, 6, 2…\n\n\nAs we can see, there are many many variables (25 columns) with many many observations (33351). Some are quite straight-forward and the name is clear (“essround”, “age”) and some much less. Sometimes we can guess the meaning of a variable’s name. But most of the time – either because guessing is too annoying or because the abbreviation is not making any sense – we need to turn to the documentation of the data set. You can find the documentation of this specific version of the data set in an html-file on Moodle (session 2).\nEvery (good and serious) data set has some sort of documentation somewhere. If not, it is not a good data set and I am even tempted to say that we should be careful in using it! The documentation for data sets is called a code book. Code books are sometimes well crafted documents and sometimes just terrible to read. In this class, you will be exposed to both kinds of code books in order to familiarize you with both.\nIn fact, this dataframe still contains many variables which we either won’t need later on or that are simply without any information. Let’s get rid of these first. This is a step which you can also do later on but I believe that it is smart to this right at the beginning in order to have a neat and tidy data set from the very beginning.\nYou can select variables (select()) right at the beginning when importing the csv file.\n\ness &lt;- read_csv(\"ESS_10_fr.csv\")  |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  )\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nHowever, I am realizing that when looking at the number of rows that my file is a bit too large for only one wave and only one country. By inspecting the ess$cntry variable, I can see that I made a mistake while downloading the dataset because it contains all countries of wave 10 instead of just one. We can fix this really easily when importing the dataset:\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  dplyr::select(\n    cntry,\n    polintr,\n    trstplt,\n    trstprt,\n    vote,\n    prtvtefr,\n    clsprty,\n    gndr,\n    yrbrn,\n    eduyrs,\n    emplrel,\n    uemp12m,\n    uemp5yr\n  ) |&gt;\n  filter(cntry != \"FR\")\n\nRows: 33351 Columns: 25\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): name, proddate, cntry\ndbl (22): essround, edition, idno, dweight, pspwght, pweight, anweight, prob...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis only leaves us with the values for France!\n\n2.3.0.1 Cleaning our DV\nAt this point, you should all check out the codebook of this data set and take a look at what the values mean. If we take the variable of ess$vote for example, we can see that there are many numeric values of which we can make hardly any sense (without guessing and we don’t do this over here) of what they might stand for.\n\nsummary(ess$vote) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.415   2.000   9.000 \n\n# remember that you can summary() both dataframes and individual variables\n\nOr in a table containing the amount of times that a value was given:\n\ntable(ess$vote)\n\n\n    1     2     3     7     8     9 \n22504  6542  1939   179   165    45 \n\n\nHere we can see that the variable vote contains the numeric values of 1 to 3 and then 7, 8, and 9. If we take a look at the code book, we can see what they stand for:\n\n1 = yes (meaning that the respondent voted)\n2 = no (meaning that the respondent did not vote)\n3 = not eligible to vote\n7 = Refusal\n8 = Don’t know\n9 = No answer\n\nThe meaning behind the values of 7, 8, and 9 are quite common and you will find them in almost all data sets which were made out of surveys. Respondents might not want to give an answer, the answer was not able to be read, or they indicated that they did not remember.\nSpoiler alert: We will work on voter turnout this session: Therefore, we will try to see what makes people vote and what decreases the likelihood that they vote at election day. The question of our dependent variable will thus be: Has the respondent voted or not?. Mathematically, this question cannot be answered with a linear regression which uses the OLS method as the dependent variable is binary meaning that 0 = has not voted/1 = has voted. There is only two possible outcomes and the variable is not continuous (one of the assumptions of an OLS).\nBut if we were to use the variable on voting turnout as it is right now, we would neither have a binary variable nor have a reliable variable as it contains values in which we are both not interested in and that will skew our estimations strongly. In fact, we cannot do a logistic regression (logit) on variables other than binary.\nThus, we first need to transform our dependent variable. We need to get rid of unwanted values and transform the 1s and 2s in 0s and 1s.\nFirst we will get rid of the unwanted values in which we are not interested.\n\n# dplyr\ness &lt;- ess |&gt;\n  filter(!vote %in% c(3, 7, 8))\n\nHere are two other ways to do it:\n\n# this one would be in base R\ness[!vote %in% c(3, 7, 8, 9)]\n\n# using the subset() function, this returns a logical vector which elements of\n# vote are not in the set of values 3, 7, 8, or 9\ness &lt;- subset(ess, vote %in% c(3,7,8,9) == F) \n\n# Alternatively you can use the %in% function with ! operator as well like this:\ness &lt;- subset(ess, !vote %in% c(3,7,8,9))\n\nQuick check to see if we got rid of all the values:\n\ntable(ess$vote)\n\n\n    1     2     9 \n22504  6542    45 \n\n\nPerfect, now we just need to transform the ones and twos into zeros and ones. This is both out of convention and also to torture you with some more data management. Since we are interested in people who do not vote, we will code those people as 0 and those who did vote as 1.\n\ness &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0))\n\nThe mutate() function is not perfectly intuitive at first sight. Here, I use the ifelse() function within mutate() to check if vote is equal to 1, if it is then it will remain 1 and if not it will be replaced by 0.\nIn Base R, you could do it like this but I believe that the mutate() function is probably the most elegant way of doing things… It’s up to you though:\n\n# only use the ifelse() function\ness$vote &lt;- ifelse(ess$vote == 1, 1, 0)\n\n# This will leave the value of 1 at 1 and change 2 to 0 for the column vote.\ness$vote[ess$vote == 1] &lt;- 1\ness$vote[ess$vote == 2] &lt;- 0\n\nWe are this close to having finished our data management part and to being able to finally work on our model. But we still have many many variables which are as “untidy” as our initial dependent variable was. Lastly, and I promise that this is the last data wrangling part for today and that we will get to our model in a moment, we need to check in the code book if specific values that we cannot simply replace over the whole data frame are still void of interest.\nOur independent variables of interest:\n\npolitical interest (polintr): c(7:9) needs to be dropped\ntrust in politicians (trstplt): no recoding necessary (unwanted values already NAs)\ntrust in political parties (trstprt): already done\nfeeling close to a party (clsprty): transform 1/2 into 0/1, drop c(7:8)\n\ngender (gndr): transform into 0/1 and drop the no answers\nyear born (yrbrn): already done\nyears of full-time education completed (eduyrs): already done\n\nWe will do every single step at once now using the pipes %&gt;% (tidyverse) or |&gt; (Base R) to have a tidy and elegant way of doing everything at once.\nBelow you can see that I first filter() out the unwanted values of our dependent variable. Then open up a muatate() in which I manipulate the independent variables which we are going to need for our model. Note how you can separate the different manipulations with a comma within the same function. This renders a very clean and easy to read code. There are several different ways you could do this. The recode(), if_else() and case_when() functions are all very useful for this kind of data wrangling. For clarity, I will use case_when() here. There is a newer version of this function called case_match() but it is only a couple of months old and for now the documentation of case_when() will probably be more helpful.\ncase_when() evaluates a series of conditions and applies the corresponding transformation to each element of a vector or column in a dataset. It’s like a more flexible and vectorized version of the if…else statement, allowing for multiple if…else conditions to be checked in a single function call. This is how it reads:\n\ncase_when(\n  condition1 ~ value_if_true1,\n  condition2 ~ value_if_true2,\n  TRUE ~ default_value\n)\n\nWith case_when(), you list your rules one by one. For each rule, you start with a condition (like “score is 80 or above”), followed by a ~, and then what you want to happen if that condition is true (like “they get an ‘A’”). After listing all your specific rules, you end with a special catch-all rule: TRUE ~ x. This is like saying, “For anyone else, or any situation I didn’t mention, do this.” This TRUE ~ x part is crucial. It ensures that if none of the specific conditions match (maybe because of some unexpected situation), you have a default action ready. It’s your “in case of anything else” rule.\nNow, let’s talk about NA_integer_. In R, NA means “not available” or missing information. But R is very particular about the kind of data it deals with. If your variable is numeric, R wants to know that even your missing pieces should be numbers. Using NA_integer_is your way of telling R, “This is a missing piece of information, but if it were here, it would be a number.” It helps keep everything organized and avoids confusion when R is looking through your data. There are also other instances (when recoding character strings for example), where you would use NA_character_ instead. Pay attention that if you have numeric values with decimal points, you will have to use NA_real_ instead of NA_integer_.\n\ness_final &lt;- ess |&gt; \n  # Filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \nmutate(\n    # Recode 'polintr' to drop values 7 to 9\n    polintr = case_when(\n      polintr %in% 7:9 ~ NA_integer_,\n      TRUE ~ polintr\n    ),\n    # Recode 'clsprty' 1/2 into 0/1 and drop 7, 8\n    clsprty = case_when(\n      clsprty == 1 ~ 0,\n      clsprty == 2 ~ 1,\n      clsprty %in% c(7, 8) ~ NA_integer_,\n      TRUE ~ clsprty\n    ),\n    # Recode 'gndr' into 0/1 and handle other cases as NAs\n    gndr = case_when(\n      gndr == 1 ~ 0,\n      gndr == 2 ~ 1,\n      TRUE ~ NA_integer_\n    ),\n    # Recode 'vote' as binary 1 (voted) & 0 (abstention)\n    vote = case_when(\n      vote == 1 ~ 1,\n      TRUE ~ 0\n    )\n  )\n\n\n2.3.0.2 Optional quick/fancy data cleaning\nEdit: I have added a more sophisticated way of cleaning all your variables in one go. I have realized during the first session of session 2 that this might be too overwhelming but I would still like to keep this way of doing things to show you how you can do it in the future. If you wish to stick to the slightly longer but maybe also clearer way above, that is absolutely fine with me! Chose the way that you understand and feel comfortable with. As always, there are a lot of different paths to the same goal in R!\n\n# specify a string of numbers we are absolutely certain we won't need\nunwanted_numbers &lt;- c(66, 77, 88, 99, 7777, 8888, 9999)\n\n# make sure to create a new object/data frame; if you don't and re-run your code\n# a second time, it will transform some mutated values again!\ness_final &lt;- ess |&gt; \n  # filtering the dependent variable to get rid of any unnecessary rows\n  filter(!vote %in% c(3, 7, 8, 9)) |&gt; \n  # mutate allows us to transform values within variables into other \n  # values or NAs\n  # vote as binary 1 (voted) & 0 (abstention)\n  mutate(across(c(polintr, clsprty), ~replace(., . %in% c(7:9), NA)),\n         across(everything(), ~replace(., . %in% unwanted_numbers, NA)),\n         vote = ifelse(vote == 1, 1, 0),\n         # recode the variable to 0 and 1\n         clsprty = recode(clsprty, `1` = 1, `2` = 0),\n         # same for gender\n         gndr = recode(gndr, `1` = 0, `2` = 1))\n\n\n2.3.1 Constructing the logit-model\nIf you have made it this far and still bear with me, you have made it to the fun part! Specifying the model and running it, literally only takes one line of code (or two depending on the amount of independent variables). And as you can see, it is really straightforward. The glm() function stands for generalized linear model and comes with Base R.\nIn Professor Rovny’s lecture, we have seen that for a Maximum Likelihood Estimation (MLE) you need to know or have an assumption about the distribution of your dependent variable. And according to this distribution, you need to find the right linear model. If you have a binary outcome, your distribution is binomial. Within the function, we thus specify the family of the distribution as such. Note that you could also specify other families such as Gaussian, poisson, gamma or many more. We are not going to touch further on that but the glm() function is quite powerful. We can specify, within the family = argument, that we are doing a logistic regression. This can be done by adding link = logit to the argument. If ever you wanted to be precise and call a probit or cauchy link, it is here that you can specify this. The standard, however, is set to logit, so we would technically not be forced to specify it in this case.\nIn terms of the model we are building right now, it follows the idea that voting behavior (voted/not-voted) is a function of political interest, trust in politicians, trust in parties, feeling close to a specific party, as well as usual control variables such as gender, age, and education:\nBy no means is this regression just extensive enough to be published. It is just one example in which I suspect that political interest, trust in politics and politicians, and party affiliation are explanatory factors.\n\nlogit &lt;- glm(\n  vote ~ polintr + trstplt + trstprt + clsprty + gndr +\n    yrbrn + eduyrs,\n  data = ess_final,\n  family = binomial(link = logit)\n)\n\nThe object called logit contains our model with its coefficients, confidence intervals and many more things that we will play with! But as you can see, the actual construction of the model is more than simple…\n\nlibrary(broom)\ntidy(logit)\n\n# A tibble: 8 × 5\n  term          estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  3.71      0.0835       44.4   0        \n2 polintr     -0.694     0.0193      -35.9   3.37e-282\n3 trstplt      0.00546   0.00253       2.16  3.08e-  2\n4 trstprt     -0.00521   0.00220      -2.37  1.78e-  2\n5 clsprty     -0.906     0.0347      -26.1   3.80e-150\n6 gndr         0.119     0.0307        3.88  1.07e-  4\n7 yrbrn       -0.0000156 0.0000262    -0.595 5.52e-  1\n8 eduyrs       0.00786   0.00162       4.85  1.27e-  6\n\n\nYou have seen both the broom package as well as stargazer in the last session.\n\nstargazer::stargazer(\n  logit,\n  type = \"text\",\n  dep.var.labels = \"Voting Behavior\",\n  dep.var.caption = c(\"Voting turnout; 0 = abstention | 1 = voted\"),\n  covariate.labels = c(\n    \"Political Interest\",\n    \"Trust in Politicians\",\n    \"Trust in Parties\",\n    \"Feeling Close to a Party\",\n    \"Gender\",\n    \"Year of Birth\",\n    \"Education\"\n  )\n)\n\n\n===================================================================\n                         Voting turnout; 0 = abstention | 1 = voted\n                         ------------------------------------------\n                                      Voting Behavior              \n-------------------------------------------------------------------\nPolitical Interest                       -0.694***                 \n                                          (0.019)                  \n                                                                   \nTrust in Politicians                      0.005**                  \n                                          (0.003)                  \n                                                                   \nTrust in Parties                          -0.005**                 \n                                          (0.002)                  \n                                                                   \nFeeling Close to a Party                 -0.906***                 \n                                          (0.035)                  \n                                                                   \nGender                                    0.119***                 \n                                          (0.031)                  \n                                                                   \nYear of Birth                             -0.00002                 \n                                         (0.00003)                 \n                                                                   \nEducation                                 0.008***                 \n                                          (0.002)                  \n                                                                   \nConstant                                  3.709***                 \n                                          (0.084)                  \n                                                                   \n-------------------------------------------------------------------\nObservations                               28,347                  \nLog Likelihood                          -13,535.140                \nAkaike Inf. Crit.                        27,086.280                \n===================================================================\nNote:                                   *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\n\n2.3.2 Interpretation of a logistic regression\nInterpreting the results of a logistic regression can be a bit tricky because the predictions are in the form of probabilities, rather than actual outcomes. This sounds quite abstract and you are right, it is abstract. However, with a proper understanding of the coefficients and odds ratios, you can gain insights into the relationship between your independent variables and the binary outcome variable even without transforming your coefficients into more easily intelligible values.\nFirst the really boring and technical definition: The coefficients of a logistic regression model represent the change in the log-odds of the outcome for a one-unit change in the predictor variable (holding all other predictors constant). The sign of the coefficient indicates the direction of the association: positive coefficients indicate that as the predictor variable increases, the odds of the outcome also increase, while negative coefficients indicate that as the predictor variable increases, the odds of the outcome decrease.\nThe odds ratio, which can be calculated from the coefficients and we will see how that works in a second (exponentation is the key word), represents the ratio of the odds of the outcome for a particular value of the predictor variable compared to the odds of the outcome for a reference value of the predictor variable. An odds ratio greater than 1 indicates that the predictor variable is positively associated with the outcome, while an odds ratio less than 1 indicates that the predictor variable is negatively associated with the outcome.\nIt’s also important to keep in mind that a logistic regression model makes assumptions about the linearity, independence and homoscedasticity of the data, if these assumptions are not met it can affect the model’s performance and interpretation. We will see the diagnostics of logistic regression models again next session.\nIs this really dense and did I lose you? It is dense but I hope you bear with me because we will see that it becomes much clearer once we apply this theory to our model but also once we exponentiate the coefficients (reversing the logarithm so to speak) and interpret them as odds-ratios.\nBut from a first glimpse at our model summary we can see that political interest, trust in politicians, closeness to a party, age and education are all statistically significant, meaning that their p-value is &lt;.05! I will not regard any other variable that is not statistically significant as you do not usually interpret non-significant variables.\nNext, we can already say that the association of interest, closeness to party and age with voting behavior is negative. This is quite logical and makes sense in our case. If we look at the scales on which these variables are coded (code book!), we can see that the higher the value of the variable, the less interested, close or aged the respondents were. Thus, it decreases their likelihood to vote on voting day. Trust in politicians is coded the other way around. If I had been a little more thorough, it would have been good to put each independent variable on the same scale… But it means that trust in politicians (in fact meaning that they trust them less) raises the likelihood of not voting somehow (positive association).\n\n2.3.2.1 Odds-ratio\nIf you exponentiate the coefficients of your model, you can interpret them as odds-ratios. Odds ratios (ORs) are often used in logistic regression to describe the relationship between a predictor variable and the outcome. ORs are easier to interpret than the coefficients of a logistic regression because they provide a measure of the change in the odds of the outcome for a unit change in the predictor variable.\nAn OR greater than 1 indicates that an increase in the predictor variable is associated with an increase in the odds of the outcome, and an OR less than 1 indicates that an increase in the predictor variable is associated with a decrease in the odds of the outcome.\nThe OR can also be used to compare the odds of the outcome for different levels of the predictor variable. For example, an OR of 2 for a predictor variable means that the odds of the outcome are twice as high for one level of the predictor variable compared to another level. Therefore, odds ratios are often preferred to coefficients for interpreting the results of a logistic regression, especially in applied settings.\nI will try to rephrase this and make it more accessible so that odds-ratios maybe become more intelligible (they are really nasty statistical stuff):\nImagine you’re playing a game where you have to guess whether a coin will land on heads or tails. If the odds of the coin landing on heads is the same as the odds of it landing on tails, then the odds-ratio would be 1. This means that the chances of getting heads or tails are the same. But if the odds of getting heads is higher than the odds of getting tails, then the odds-ratio would be greater than 1. This means that the chances of getting heads is higher than the chances of getting tails. On the other hand, if the odds of getting tails is higher than the odds of getting heads, then the odds-ratio would be less than 1. This means that the chances of getting tails is higher than the chances of getting heads. In logistic regression, odds-ratio is used to understand the relationship between a predictor variable (let’s say “X”) and an outcome variable (let’s say “Y”). Odds ratio tells you how much the odds of Y happening change when X changes.\nSo, for example, if the odds ratio of X is 2, that means that if X happens, the odds of Y happening are twice as high as when X doesn’t happen. And if the odds ratio of X is 0.5, that means that if X happens, the odds of Y happening are half as high as when X doesn’t happen.\n\n# simply use exp() on the coefficients of the logit\nexp(coef(logit))\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n# here would be a second way of doing it\nexp(logit$coefficients)\n\n(Intercept)     polintr     trstplt     trstprt     clsprty        gndr \n 40.8120705   0.4997134   1.0054777   0.9948009   0.4040527   1.1261108 \n      yrbrn      eduyrs \n  0.9999844   1.0078943 \n\n\nWe can also, and should, add the 95% confidence intervals (CI). As a quick reminder, the CI is a range of values that is likely to contain the true value of a parameter (the coefficients of our predictor variables in our case). This comes at a certain level of confidence. The most commonly used levels (attention, this is only a statistical convention!) of confidence are 95% and sometimes 99%.\nA 95% CI for a parameter, for example, means that if the logistic regression model were fitted to many different samples of data, the true value of the parameter would fall within the calculated CI for 95% of those samples.\n\n# most of the times the extra step in the next lines is not necessary and this \n# line of code is enough\nexp(cbind(OR = coef(logit), confint(logit)))\n\nWaiting for profiling to be done...\n\n\n                    OR      2.5 %     97.5 %\n(Intercept) 40.8120705 34.6333002 48.0596000\npolintr      0.4997134  0.4810885  0.5189496\ntrstplt      1.0054777  1.0005481  1.0105282\ntrstprt      0.9948009  0.9905477  0.9991390\nclsprty      0.4040527  0.3773609  0.4323782\ngndr         1.1261108  1.0604502  1.1958345\nyrbrn        0.9999844  0.9999340  1.0000369\neduyrs       1.0078943  1.0047438  1.0111607\n\n# here, however, we must combine both the exponentiate coefficients with the 95% confidence intervals\n# the format() function, helps me to show the numbers without the exponentiated \n# \"e\" and without scientific notation; the round() function within this function gives me values which are rounded on the 5th decimal place.\nformat(round(exp(cbind(\n  OR = coef(logit), confint(logit)\n)), 5),\nscientific = FALSE, digits = 4)\n\nWaiting for profiling to be done...\n\n\n            OR        2.5 %     97.5 %   \n(Intercept) \"40.8121\" \"34.6333\" \"48.0596\"\npolintr     \" 0.4997\" \" 0.4811\" \" 0.5190\"\ntrstplt     \" 1.0055\" \" 1.0006\" \" 1.0105\"\ntrstprt     \" 0.9948\" \" 0.9906\" \" 0.9991\"\nclsprty     \" 0.4041\" \" 0.3774\" \" 0.4324\"\ngndr        \" 1.1261\" \" 1.0604\" \" 1.1958\"\nyrbrn       \" 1.0000\" \" 0.9999\" \" 1.0000\"\neduyrs      \" 1.0079\" \" 1.0047\" \" 1.0112\"\n\n\nThis exponentiated value, the odds ratio (OR), now allows us to say that for a one unit increase in political interest, for example, the odds of voting (versus not voting) decrease. The same goes for the other variables.\nThe last thing about odds-ratio and I hope that this is the easiest to interpret, is when you try to make percentages out of it:\n\n# the [-1] drops the value of the intercept as it is statistically meaningless\n# we put another minus one to get rid of 1 as a threshold for interpreting the\n# odds-ratio\n# we multiply by 100 to have percentages\n100*(exp(logit$coefficients[-1])-1)\n\n      polintr       trstplt       trstprt       clsprty          gndr \n-50.028658955   0.547769108  -0.519908142 -59.594726196  12.611084187 \n        yrbrn        eduyrs \n -0.001560893   0.789429192 \n\n\nThis allows us to say that being politically uninterested decreases the odds of voting by 35%. Much more straightforward right?\n\n2.3.2.2 Predicted Probabilities\nPredicted probabilities also allow us to understand our logistic regression. In logistic regressions, the predicted probabilities and ORs are two different ways of describing the relationship between the predictor variables and the outcome. Predicted probabilities refer to the probability that a specific outcome will occur, given a set of predictor variables. They are calculated using the logistic function, which maps the linear combination of predictor variables (also known as the log-odds) to a value between 0 and 1.\nThe importance here is that we chose the predictor variables and at which values of those we are trying to predict the outcome. This is what we call “holding independent variables constant” while we calculate the predicted probability for a specific independent variable of interest.\nI will repeat this to make sure that everybody can follow along. With the predicted probabilities, we are trying to make out the effect of one specific variable of interest on our dependent variable, while we hold every other variable at their mean, median in some cases or, in the case of a dummy variable, at one of the two possible values. By holding them constant, we can be sure to see the singular effect of our independent variable of interest.\nIn our case, let “feeling close to a party” (1 = yes; 0 = no) be our independent variable of interest. We take our old ess_final dataframe and create a new one. In the newdata dataframe, we hold all values at their respective means or put our binary/dummy variables to 1. It is an arbitrary choice to put it to one here. We could also put it to 0. The only variable that we allow to alternate freely to find the predicted probabilities is our variable of interest clsprty.\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = mean(polintr),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = c(0, 1),\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n    ))\n\nIf that all worked out, we can predict the values for this specific independent variable by using the Base R predict() function:\n\nnewdata$preds &lt;- predict(logit, newdata = newdata, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata, aes(x = clsprty, y = preds)) +\n  geom_line() +\n  ylab(\"Likelihood of Voting\") + xlab(\"Feeling Close to a Party\")\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nWe can also do the same thing to see the predicted probability of political interest on voting behavior. This is a bit more interesting as the variable is not binary like ess$clsprty:\n\n# creating the new dataframe newdata with the old dataframe ess_final\nnewdata_1 &lt;- with(\n  # the initial dataframe contains NAs, we must get rid of them!\n  na.omit(ess_final),\n  # construct a new dataframe\n  data.frame(\n    # hold political interest at its mean\n    polintr = c(1:4),\n    # hold trust in politicians at its mean\n    trstplt = mean(trstplt),\n    # hold trust in parties at its mean\n    trstprt = mean(trstprt),\n    # let it vary on our IV of interest\n    clsprty = 1,\n    # gender is set to 1\n    gndr = 1,\n    # mean of age\n    yrbrn = mean(yrbrn),\n    # mean of education\n    eduyrs = mean(eduyrs)\n  )\n)\n\n\nnewdata_1$preds &lt;- predict(logit, newdata = newdata_1, type = \"response\")\n\nNow, let’s plot the values:\n\nggplot(newdata_1, aes(x = polintr, y = preds)) +\n  geom_line() +\n  ylab(\"predicted probability\") + xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n#combines value data frame created above with predicted probabilities evaluated \n# at the data values\nnewdata_1 &lt;-\n  cbind(newdata_1,\n        predict(\n          logit,\n          newdata = newdata_1,\n          type = \"link\",\n          se = TRUE\n        )) \n\n\nnewdata_1 &lt;- within(newdata_1, {\n  pp &lt;- plogis(fit)                   # predicted probability\n  lb &lt;- plogis(fit - (1.96 * se.fit)) # builds lower bound of CI\n  ub &lt;- plogis(fit + (1.96 * se.fit)) # builds upper bound of CI\n})\n\n\nggplot(newdata_1, aes(x = polintr, y = pp)) +\n  geom_line(aes(x = polintr, y = pp, color = as.factor(gndr))) +\n  geom_ribbon(aes(ymin = lb, ymax = ub), alpha = 0.3) +\n  theme(legend.position = \"none\") +\n  ylab(\"predicted probability to abstain from voting\") +\n  xlab(\"political interest\")\n\nWarning: Removed 4 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\n\n2.3.3 Making life easiest\nYou are going to hate me if I tell you that all these steps which we just computed by hand… can be done by using a package. This is only 0.01% of me trying to be mean but mostly because it is extremely helpful and necessary to understand what is going on under the hood of predicted probabilities. The interpretation of logistic regressions is tricky and if you do not know what you are computing, it is even more complicated.\nWorking with packages is great, and I am aware that I always encourage you to use packages that make your life easier. But and this is an important “but” we do not always understand what is going on under the hood of a package. It is like putting your logistic regression into a black box, shaking it really well, and then taking a look at the output and putting it on shaky interpretational terms.\nBut enough of personal defense, as to why I made you suffer through all this. Here is my code to do most of the steps at once:\n\n# this package contains everything we need craft predicted probabilities and\n# visualize them as well\nlibrary(ggeffects)\n\n# like the predcit() function of Base R, we use ggpredict() and specify\n# our variable of interest\ndf &lt;- ggpredict(logit, terms = \"polintr\")\n\n# this is the simplest way of plotting this\nggplot(df, aes(x = x, y = predicted)) +\n  # our graph is more or less a line, so geom_line() applies\n  geom_line() +\n  # geom_ribbon() with the values that ggpredict() provided for the confidence\n  # intervals then gives\n  # us a shade around the geom_()line as CIs\n  geom_ribbon(aes(ymin = conf.low, ymax = conf.high), alpha = .1)\n\n\n\n\n\n\n\nAnd voilà, your output it less than 10 lines of code.",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#automating-things-in-r-optional",
    "href": "session2/session2.html#automating-things-in-r-optional",
    "title": "\n2  Logistic Regressions\n",
    "section": "\n2.4 Automating things in R (OPTIONAL!)",
    "text": "2.4 Automating things in R (OPTIONAL!)\nWhen programming, it usually takes time to understand and apply things. The next step should often be to think about how to automate something in order to make processes faster and more elegant. Once we have understood a process relatively well, we can apply it to other areas in an automated way relatively easily.\nFor example, let’s think about our logistical regressions today. We worked with the ESS and looked at the 10th round of the data set for France. Our model aimed to investigate which variables can predict abstention. But I can also ask myself this question over a certain period of time, i.e. over several waves of the ESS, or for several countries. If I now tell you that you should do the logit for all countries of the ESS, the first step would be to run my code for n = countries of the ESS. However, this would result in a very long script, would not be very elegant and the longer the code, the higher the probability of errors.\nIf you are programming and realize that you have to do the same steps several times and it is actually the same step, only that a few parameters (such as the names of variables) change, then you can be sure that this could also be automated. At a certain point in programming, the goal should always be to create a pipeline for the work steps, which has the goal of making our code run as automatically as possible.\nIn RStudio we have several ways to automate this. For example, you can write so-called for-loops, which perform certain operations one after the other for certain list entries. Or you can write your own functions. At some point, someone decided to write the function glimpse() or read_csv2(), for example. As private users of R, we can do the same. In this way, we can, for example, accommodate several operations within a function that we write ourselves, which can then be applied simultaneously to several objects or a data set with different countries.\nI am aware that this is only the second session and that may sound like a lot. Everything from here on is optional, but I think it’s important that you see this as early as possible. Some of you may already feel comfortable enough to try something like this out for yourselves. If you don’t, that’s okay too. You will get there, trust me! I just want to show you what is possible and what you can do with R.\n\n\n\n\n\n2.4.1 Writing your function\nFunctions in R are incredibly powerful and essential for efficient and automated programming. A function in R is defined using the function keyword. The basic structure includes the name of the function, a set of parameters, and the body where the actual computations are performed.\nThe basic syntax is as follows:\nThere are some minor conventions in RStudio when writing functions. Some of them also apply to other parts than just functions.\n\nYou should give them some “breathing” space. When you write the accolades, put a space bar in between.\n\n\n# this is bad\nfunction(x){\n  x + 1\n}\n\n\n# this is good\nfunction(x) {\n  x + 1\n}\n\n\nYou should always put a space between the equal sign and the value you assign to a variable. Place spaces around all infix operators (=, +, -, &lt;-, etc.)\n\n\n# this is bad\ndata&lt;-read.csv(\"data.csv\")\n\n\ndata &lt;- read.csv(\"data.csv\")\n\n\n“stretch” your code when possible ctrl + shift + a can be of help for that\n\n\n# this is bad but in a longer example\ncountry_model &lt;- function(df) {glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, family = binomial(link = \"logit\"), data = df)\n}\n\n\n# this is better\ncountry_model &lt;- function(df) {\n  glm(\n    vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs,\n    family = binomial(link = \"logit\"),\n    data = df\n  )\n}\n\n\nWe usually assign verbs to functions. This means that the name of the function should be a verb that describes what the function does. If we want to create a function that reads the ESS files and does several operations at once, we should call it something like read_ess().\n\n2.4.2 The purrr package\nThe purrr package in R is a powerful tool that helps in handling repetitive tasks more efficiently. It’s part of the Tidyverse, a collection of R packages designed to make data science faster, easier, and more fun!\nIn simple terms, purrr improves the way you work with lists and vectors in R. It provides functions that allow you to perform operations, i.e. pre-existing functions or functions you will write yourselves, on each element of a list or vector without writing explicit loops. This concept is known as functional programming.\n\n\n\n\n\n\nWhy use purrr instead of for-loops?\n\n\n\n\n\n\nSimplifies Code: purrr makes your code cleaner and more readable. Instead of writing several lines of loop code, you can achieve the same with a single line using purrr.\nConsistency and Safety: purrr functions are consistent in their behavior, which reduces the chances of errors that are common in for loops, like mistakenly altering variables outside the loop.\nHandles Complexity Well: When working with complex or nested lists, purrr offers tools that make these tasks much simpler compared to traditional loops.\nIntegration with tidyverse: Since purrr is part of the tidyverse, it integrates smoothly with other Tidyverse packages, making your entire data analysis workflow more efficient.\n\n\n\n\nPut simply, we can use the functions of the purrr package to apply a function to each element of a list or vector. Depending on the specific purrr-function, our output can be different but we can also specify the output in our manually written function which we feed into purrr.\n\n2.4.3 The map() function\nThe map() function, part of the purrr package in R, is built around a simple yet powerful concept: applying a function to each element of a list or vector and returning a new list with the results. This concept is known as “mapping,” hence the name map().\n\n\nThe logic is simple. You take map(a list of your choice + your function) which then creates an output that behaves as if you had applied that function to each list entry individually.\n\nHere’s a breakdown of the logic behind map():\n\nInput: The primary input to map() is a list or a vector. This could be a list of numbers, characters, other vectors, or even more complex objects like data frames.\nFunction Application: You specify a function that you want to apply to each element of the list. This function could be a predefined function in R, or a custom function you’ve written. The key is that this function will be applied individually to each element of your list/vector.\nIteration: map() internally iterates over each element of your input list/vector. You don’t need to write a loop for this; map() handles it for you. For each element, map() calls the function you specified.\nOutput: For each element of the list/vector, the function’s result is stored. map() then returns a new list where each element is the result of applying the function to the corresponding element of the input.\nFlexibility in Output Type: The purrr package provides variations of map() to handle different types of output. For example, map_dbl() if your function returns doubles, map_chr() for character output, and so on. This helps in ensuring that the output is in the format you expect.\n\n2.4.4 Automatic regressions for several countries\nThis is absolutely only optional. I do not ask you to reproduce anything of this at any point in this class. I simply wanted to show you what you can do in R and what I mean when I say that automating stuff makes life easier.\nI present you here with a code that does the logistic regression we have been doing but on all countries of the ESS at the same time and then plots us the odds-ratio of our variable of interest “political interest”, as well as comparing McFadden pseudo R2 (we’ll see this term next session again).\nI will combine things from the optional section of Session 1 and the purrr package to do this. The idea is to build one model per country of the ESS, nest() it in a new tibble (What are tibbles again?) 1 where each row contains the information necessary for one country-model and to then use map() to apply the function tidy() from the broom package to each of these models.\nBelow, you can find a simple function that takes a dataframe as input and returns a logistic regression model. Within, I only specify the glm() function which I have shown you above. Within the parentheses of function() I specify the name of the input object. This name is arbitrary and can be anything you want. I chose df for dataframe. It has to appear somewhere within your function later on; usually there where your operation on the input object is supposed to be performed. In my case this is data = df.\n\ncountry_model &lt;- function(df) {\n  glm(vote ~ polintr + trstplt + trstprt + clsprty + gndr + yrbrn + eduyrs, \n      family = binomial(link = \"logit\"), data = df)\n}\n\n\nI will first talk you through the different steps and then provide you one long pipeline that does all this in one step.\n\nFirst, I import the ESS data and select the variables I want to use. I also clean the data a bit and get rid of unwanted observations or values.\n\n# Recoding the 'vote' variable to binary (1 or 0)\n# and filtering the dataset based on specified criteria\nprepared_ess &lt;- ess |&gt; \n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt; \n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  )\n\nThen we will nest() the data as described here where I explain the logic of nesting.\n\n# library for McFadden pseudo R2\nlibrary(pscl)\nlibrary(broom)\n\ness &lt;- read_csv(\"ESS_10_fr.csv\") |&gt;\n  select(cntry,\n         vote,\n         polintr,\n         trstplt,\n         trstprt,\n         clsprty,\n         gndr,\n         yrbrn,\n         eduyrs)\n\ness_model &lt;- ess |&gt; \n  as_tibble() |&gt;  # Convert the data frame to a tibble\n  mutate(vote = ifelse(vote == 1, 1, 0)) |&gt;\n  filter(\n    vote %in% c(0:1),\n    polintr %in% c(1:4),\n    clsprty %in% c(1:2),\n    trstplt %in% c(0:10),\n    trstprt %in% c(0:10),\n    gndr %in% c(1:2),\n    yrbrn %in% c(1900:2010),\n    eduyrs %in% c(0:50)\n  ) |&gt;\n  group_by(cntry) |&gt;\n  nest() |&gt;\n  mutate(\n    model = map(data, country_model),\n    tidied = map(model, ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE)),\n    glanced = map(model, glance),\n    augmented = map(model, augment),\n    mcfadden = map(model, ~ pR2(.x)[4])\n  )\n\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\nfitting null model for pseudo-r2\n\n\n\n# Comparing AICs\npR2(logit)\n\nfitting null model for pseudo-r2\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1.353514e+04 -1.520203e+04  3.333773e+03  1.096490e-01  1.109536e-01 \n         r2CU \n 1.686556e-01 \n\ness_model |&gt;\n  unnest(mcfadden) |&gt;\n  ggplot(aes(fct_reorder(cntry, mcfadden), mcfadden)) +\n  geom_col() + coord_flip() +\n  scale_x_discrete(\"Country\")\n\n\n\n\n\n\n\n\n# Comparing coefficients\ness_model |&gt;\n  unnest(tidied) |&gt;\n  filter(term == \"polintr\") |&gt;\n  ggplot(aes(\n    reorder(cntry, estimate),\n    y = exp(estimate),\n    color = cntry,\n    ymin = exp(conf.low),\n    ymax = exp(conf.high)\n  )) +\n  geom_errorbar() +\n  geom_point() +\n  scale_x_discrete(\"Country\") +\n  ylab(\"Odds-Ratio of political interest\") +\n  xlab(\"Country\")",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session2/session2.html#footnotes",
    "href": "session2/session2.html#footnotes",
    "title": "\n2  Logistic Regressions\n",
    "section": "",
    "text": "The quick answer is that tibbles are a modern take on data frames in R, offering improved printing (showing only the first 10 rows and fitting columns to the screen), consistent subsetting behavior (always returning tibbles), tolerance for various column types, support for non-standard column names, and no reliance on row names. They represent a more adaptable, user-friendly approach for handling data in R, especially suited for large, complex datasets.↩︎",
    "crumbs": [
      "Logistic Regression",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Logistic Regressions</span>"
    ]
  },
  {
    "objectID": "session3/session3.html",
    "href": "session3/session3.html",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "3.1 Introduction\nIn this script, I will show you how to construct a multinomial logistic regression in R. For this, we will work on the European Social Survey (ESS) again. These are the main points that are covered in this script:",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#introduction",
    "href": "session3/session3.html#introduction",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The logic of multinomial (logistic) regressions\nAdvanced Data Management\nInterpretation of a multinomial Model\nModel Diagnostics\nGoodness of Fit\nAPIs and Data Visualization (OPTIONAL!)",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "href": "session3/session3.html#the-logic-of-multinomial-logistic-regressions",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.2 The Logic of multinomial (logistic) Regressions",
    "text": "3.2 The Logic of multinomial (logistic) Regressions\nI have chosen four countries out of which you will be able to choose one later one when I ask you to work on some exercises. For now, I will mainly work on Germany. One of the classic applications of multinomial models in political science is the question of voting behavior, more precisely vote choice. Last week, we have seen models of a logistic regression (logit). It is used in cases when our dependent variable (DV) is binary (0 or 1; true or false; yes or no) which means that we are not allowed to use OLS. The idea of logit can be extended to unordered categorical or nominal variables with more than two categories, e.g.: Vote choice, Religion, Brands…\nInstead of one equation modelling the log-odds of \\(P(X=1)\\), we do the same thing but for the amount of categories that we have. In fact, this means that a multinomial model runs several single logistic regressions on something we call a baseline. R will choose this baseline to which the categorical values of our DV will then relate. But we can also change it (this is called releveling). This allows us to make very interesting inferences with categorical (or ordinal) variables. If this sounds confusing, you should trust me when I tell you that this will become more straightforward in a second!\nHowever, this also makes the interpretation of these models a bit intricate and opaque at times. Nevertheless, you will see that once you have understood the basic idea of a multinomial regression and how to interpret the values in accordance to the baseline, it is not much different from logistic regressions on binary variables (and in my eyes even a bit simpler…). If the logic of logit is not 100% clear at this point, I recommend you go back to last session’s script on logit and work through my explanations. And if that does not help, try to follow this lecture attentively. As I said, the logic is the same, so I will repeat myself :) And if it is still unclear, you can always ask in class or come see me after the session!\nBut enough small talk, let’s first do some data wrangling which you all probably dread at this point…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#data-management-for-multinomial-regression",
    "href": "session3/session3.html#data-management-for-multinomial-regression",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.3 Data Management for Multinomial Regression",
    "text": "3.3 Data Management for Multinomial Regression\nAs I have said, we will work on voting choice in four different countries. I selected Denmark and Germany. Germany I have chosen because I was working on this model a couple of months ago and Denmark is for fun.\nThe data which we will use for this session is the 9th round of the ESS published in 2018. The goal of this session is to understand predictors that tell us more about why people vote for Populist Radical-Right Parties, henceforth called PRRP (Mudde 2007). For this I have two main hypotheses in mind, as well as some predictors which I know are important based on the literature. Finally we also need some control variables which we need to control for in almost any regression analysis using survey data.\nMy two hypotheses (H1) and H2) are as follows:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nNow you might notice two things. First, my hypotheses are relatively self-explanatory and you are absolutely right. They are more than that, they are perhaps even self-evident. But to this, I would just reply that this is supposed to be an easy exercise which is supposed to expose you to a multinomial regression and the logic of it. Second, you might see that my hypotheses are relatively broadly formulated. This is because I would like you, later in class, to choose one of the countries of the 9th wave of the ESS and build a model yourselves. By giving you broad hypotheses, you can do this ;)\n\n# read_csv from the tidyverse package\ness &lt;- read_csv(\"ESS9e03_1.csv\") |&gt; \n  # dplyr allows me to select only those variables I want to use later\n  select(cntry, \n         prtvtdfr, \n         prtvede1, \n         prtvtddk, \n         prtvtdpl, \n         imueclt, \n         yrbrn, \n         eduyrs, \n         hinctnta, \n         stflife, \n         trstplt, \n         blgetmg, \n         gndr) |&gt; \n  # based on the selected variables, I filter the dataframe so that I am only\n  # left with the data for Germany and Denmark\n  filter(cntry %in% c(\"DE\", \"DK\"))\n\nRows: 49519 Columns: 572\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (10): name, proddate, cntry, ctzshipd, cntbrthd, lnghom1, lnghom2, fbrn...\ndbl (562): essround, edition, idno, dweight, pspwght, pweight, anweight, pro...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, every transformation and mutation of variables which you see below is done based on my knowledge of the dataset which I solely gained from looking at the code book. The code book can be found on the Moodle page (or the ESS’ website). It is highly important that you get used to reading a code book in general but especially to familiarize yourselves with the data which you will use by looking at the way that the variables are coded in the code book. There, for example, you will find information on the numeric values which are stored in the variables prtvtdfr, prtvede1, prtvtddk and prtvtdpl. They all stand for a category or, in our case, a party name which you can only identify if you open the code book. You will see that I only selected some parties in the mutate() function below. This is more or less to get rid of those parties that did not make it into the national parliament at the last national election of each country.\nYou have seen a similar chunk of code in the last script. See how, once you have a code that works for one dataset, you can use it again?\n\n# cleaning the dependent variables all over the dataframe\ness_clean &lt;- ess |&gt;\n    mutate(across(where(is.numeric), ~case_when(\n           . %in% c(66, 77, 88, 99, 7777, 8888, 9999) ~ NA_integer_,\n           TRUE ~ .)),\n      prtvtdfr = replace(prtvtdfr, prtvtdfr %in% c(1, 2, 10, 12:99), NA),\n           prtvede1 = replace(prtvede1, !prtvede1 %in% c(1:6), NA),\n           prtvtddk = replace(prtvtddk, !prtvtddk %in% c(1:10), NA),\n           prtvtdpl = replace(prtvtdpl, !prtvtdpl %in% c(1:8), NA),\n           # get rid of unwanted values indicating no response etc\n           blgetmg = replace(blgetmg, !blgetmg %in% c(1:2), NA),\n           # gender recoded to 1 = 0, 2 = 1 (my personal preference)\n           gndr = recode(gndr, `1` = 0, `2` = 1))\n\nIn fact, you could already build the model now and start the multinomial regression. However, I add an additional data management step by placing the numeric values of the election variable in a new variable called vote_de, where I convert the numeric values to character values and at the same time give them the names of the parties. This will automatically transform NAs in all the rows in which the country is not that in which the person has voted.\nBut more importantly, once I run the regression, it will display the parties’ names instead of the numbers. This means that I won’t have to go back to the code book every time to check what the 1s or 2s correspond to.\n\n# this is simple base R creating a new column/variable with character\n# values corresponding to the parties' names behind the numeric values\ness_clean$vote_de[ess_clean$prtvede1==1]&lt;-\"CDU/CSU\"\ness_clean$vote_de[ess_clean$prtvede1==2]&lt;-\"SPD\"\ness_clean$vote_de[ess_clean$prtvede1==3]&lt;-\"Die Linke\"\ness_clean$vote_de[ess_clean$prtvede1==4]&lt;-\"Grüne\"\ness_clean$vote_de[ess_clean$prtvede1==5]&lt;-\"FDP\"\ness_clean$vote_de[ess_clean$prtvede1==6]&lt;-\"AFD\"\n\nHere is a way to mutate all the variables at once. However, this somehow creates conflicts with a package used further below.\n\ness_clean &lt;- ess_clean |&gt; \n  mutate(\n    vote_dk = case_when(prtvtddk == 1 ~ \"Socialdemokratiet\",\n                        prtvtddk == 2 ~ \"Det Radikale Venstre\",\n                        prtvtddk == 3 ~ \"Det Konservative Folkeparti\",\n                        prtvtddk == 4 ~ \"SF Socialistisk Folkeparti\",\n                        prtvtddk == 5 ~ \"Dansk Folkeparti\",\n                        prtvtddk == 6 ~ \"Kristendemokraterne\",\n                        prtvtddk == 7 ~ \"Venstre\",\n                        prtvtddk == 8 ~ \"Liberal Alliance\",\n                        prtvtddk == 9 ~ \"Enhedslisten\",\n                        prtvtddk == 10 ~ \"Alternativet\",\n                        TRUE ~ NA_character_),\n    vote_de = case_when(prtvede1 == 1 ~ \"CDU/CSU\",\n                        prtvede1 == 2 ~ \"SPD\",\n                        prtvede1 == 3 ~ \"Die Linke\",\n                        prtvede1 == 4 ~ \"Grüne\",\n                        prtvede1 == 5 ~ \"FDP\",\n                        prtvede1 == 6 ~ \"AFD\"))",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#constructing-the-model",
    "href": "session3/session3.html#constructing-the-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.4 Constructing the Model",
    "text": "3.4 Constructing the Model\nNow that the data management process is finally over, we can specify our model. For this, you need to install the nnet package and load it to your library. Once this is done, we will take the exact same steps as you would do for an OLS or logit model. You specify your DV followed by a ~ and then you only need to add all your IVs. Lastly, you need to specify the data source. Hess = TRUE will provide us with a Hessian matrix that we need for a package later. If you don’t know what that is… that is absolutely fine!\n\nlibrary(nnet)\nmodel_de &lt;- multinom(vote_de ~ imueclt  + stflife + trstplt + blgetmg + \n                    gndr + yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  60 (45 variable)\ninitial  value 2512.046776 \niter  10 value 2043.014063\niter  20 value 2023.586615\niter  30 value 1980.080494\niter  40 value 1938.289705\niter  50 value 1927.868641\niter  60 value 1926.043042\niter  70 value 1925.949503\niter  80 value 1925.873772\nfinal  value 1925.814902 \nconverged\n\nmodel_dk &lt;- multinom(vote_dk ~ imueclt  + stflife + trstplt + blgetmg + gndr +\n                     yrbrn + eduyrs + hinctnta,\n                     data = ess_clean,\n                     Hess = TRUE)\n\n# weights:  100 (81 variable)\ninitial  value 2525.935847 \niter  10 value 2265.701974\niter  20 value 2136.112054\niter  30 value 2056.238932\niter  40 value 1999.847654\niter  50 value 1950.379411\niter  60 value 1938.124756\niter  70 value 1934.172909\niter  80 value 1915.231553\niter  90 value 1908.197162\niter 100 value 1906.641552\nfinal  value 1906.641552 \nstopped after 100 iterations\n\n\n\n3.4.1 Re-leveling your DV\nIn my case, the German PRRP is called Alternative für Deutschland meaning it starts with an “A”. R tends to take the alphabetical order as a criterion for the baseline meaning that the baseline for your multinomial model is chosen based on the party which comes first in alphabetical order. Depending on what you want to show, you might want to change the baseline which we can do with the relevel() function. Let’s say we are not interested in vote choice regarding the PRRP but conservative parties and thus want to put the German Christian conservative party, the CDU/CSU, as a baseline. Here is how we could do this in R:\n\n# don't run this code chunk\n#| eval: false\n# you need to specify your DV as a factor for this; further, the ref must \n# contain the exact character label of the party\ness_clean$vote_de &lt;- relevel(as.factor(ess_clean$vote_de), ref = \"CDU/CSU\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-model",
    "href": "session3/session3.html#interpreting-a-multinomial-model",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.5 Interpreting a Multinomial Model",
    "text": "3.5 Interpreting a Multinomial Model\nYou already know that I like the stargazer package for displaying a regression table. This time I paid attention to what level of statistical significance leads to a star (*). I changed it so that, like in the summary() function, p-values below 0.05 will be used as the minimum level of statistical significance instead of 0.1. dep.var.caption = allows be to specify a caption for our DV and we can use our own labels for the IVs instead of the variables’ names by using the covariate.labels = argument.\nI have specified in the first chunk of code which arguments concern the generated output in LaTeX. I still recommend you start learning how to write papers in LaTeX. This is just to say that some arguments are not useful at all when type = \"text. But LaTeX generates more beautiful tables ;)\n\n# specifying the object in which the model is stored\nstargazer::stargazer(\n  model_de,\n  # adding a title to the table\n  title = \"Multinomial Regression Results Germany\",\n  # change this to the desired output format; either\n  # LaTeX, html, or text (depending on your document)\n  # editor\n  type = \"html\",\n  # some LaTeX information\n  float = TRUE,\n  # font size of the LaTeX table\n  font.size = \"small\",\n  # column width in final LaTeX table\n  column.sep.width = \"-10pt\",\n  # specifying the p-values which lead to stars in our\n  # table\n  star.cutoffs = c(.05, .01, .001),\n  # caption for the DV\n  dep.var.caption = c(\"Vote Choice\"),\n  # labels for our IVs; must be in the same order as our\n  # IVs in the initial model\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Germany\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nCDU/CSU\n\n\nDie Linke\n\n\nFDP\n\n\nGrüne\n\n\nSPD\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n0.350***\n\n\n0.617***\n\n\n0.326***\n\n\n0.784***\n\n\n0.493***\n\n\n\n\n\n\n(0.063)\n\n\n(0.080)\n\n\n(0.076)\n\n\n(0.075)\n\n\n(0.065)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.029\n\n\n-0.171*\n\n\n0.095\n\n\n-0.094\n\n\n-0.109\n\n\n\n\n\n\n(0.067)\n\n\n(0.083)\n\n\n(0.093)\n\n\n(0.079)\n\n\n(0.067)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.513***\n\n\n0.281**\n\n\n0.424***\n\n\n0.352***\n\n\n0.398***\n\n\n\n\n\n\n(0.077)\n\n\n(0.092)\n\n\n(0.090)\n\n\n(0.085)\n\n\n(0.078)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n-0.054***\n\n\n-0.443***\n\n\n-1.086***\n\n\n-0.382***\n\n\n-0.385***\n\n\n\n\n\n\n(0.002)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n0.859***\n\n\n0.240*\n\n\n0.573***\n\n\n0.928***\n\n\n0.395***\n\n\n\n\n\n\n(0.103)\n\n\n(0.096)\n\n\n(0.095)\n\n\n(0.137)\n\n\n(0.112)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.013***\n\n\n0.007***\n\n\n-0.010***\n\n\n-0.002***\n\n\n-0.016***\n\n\n\n\n\n\n(0.0004)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.0004)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n0.014\n\n\n0.081\n\n\n0.036\n\n\n0.091\n\n\n0.040\n\n\n\n\n\n\n(0.051)\n\n\n(0.059)\n\n\n(0.060)\n\n\n(0.055)\n\n\n(0.052)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n0.116*\n\n\n-0.032\n\n\n0.114\n\n\n0.136*\n\n\n0.047\n\n\n\n\n\n\n(0.055)\n\n\n(0.066)\n\n\n(0.067)\n\n\n(0.062)\n\n\n(0.056)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n23.264***\n\n\n-16.715***\n\n\n17.583***\n\n\n-1.578***\n\n\n30.969***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.00005)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n3,941.630\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\n\n# the annotations of the above model would be the same for this model\nstargazer::stargazer(\n  model_dk,\n  title = \"Multinomial Regression Results Denmark\",\n  type = \"html\",\n  float = TRUE,\n  font.size = \"tiny\",\n  star.cutoffs = c(.05, .01, .001),\n  dep.var.labels = c(\"Germany\"),\n  dep.var.caption = c(\"Vote Choice\"),\n  covariate.labels = c(\n    \"Positivity Immigration\",\n    \"Satisfaction w/ Life\",\n    \"Trust in Politicians\",\n    \"Ethnic Minority\",\n    \"Gender\",\n    \"Age\",\n    \"Education\",\n    \"Income\"\n  )\n)\n\n\nMultinomial Regression Results Denmark\n\n\n\n\n\n\n\n\n\nVote Choice\n\n\n\n\n\n\n\n\n\n\n\n\nGermany\n\n\nDet Konservative Folkeparti\n\n\nDet Radikale Venstre\n\n\nEnhedslisten\n\n\nKristendemokraterne\n\n\nLiberal Alliance\n\n\nSF Socialistisk Folkeparti\n\n\nSocialdemokratiet\n\n\nVenstre\n\n\n\n\n\n\n(1)\n\n\n(2)\n\n\n(3)\n\n\n(4)\n\n\n(5)\n\n\n(6)\n\n\n(7)\n\n\n(8)\n\n\n(9)\n\n\n\n\n\n\n\n\nPositivity Immigration\n\n\n-0.667***\n\n\n-0.440***\n\n\n-0.018\n\n\n0.162*\n\n\n-0.169\n\n\n-0.403***\n\n\n0.028\n\n\n-0.238***\n\n\n-0.400***\n\n\n\n\n\n\n(0.066)\n\n\n(0.083)\n\n\n(0.084)\n\n\n(0.081)\n\n\n(0.133)\n\n\n(0.087)\n\n\n(0.084)\n\n\n(0.059)\n\n\n(0.060)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSatisfaction w/ Life\n\n\n0.153*\n\n\n0.238\n\n\n0.132\n\n\n-0.043\n\n\n0.174\n\n\n0.164\n\n\n0.071\n\n\n0.081\n\n\n0.249***\n\n\n\n\n\n\n(0.070)\n\n\n(0.131)\n\n\n(0.109)\n\n\n(0.082)\n\n\n(0.176)\n\n\n(0.125)\n\n\n(0.098)\n\n\n(0.059)\n\n\n(0.069)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrust in Politicians\n\n\n0.180**\n\n\n0.349***\n\n\n0.300***\n\n\n0.003\n\n\n0.334*\n\n\n0.298**\n\n\n0.103\n\n\n0.264***\n\n\n0.451***\n\n\n\n\n\n\n(0.068)\n\n\n(0.093)\n\n\n(0.086)\n\n\n(0.076)\n\n\n(0.144)\n\n\n(0.095)\n\n\n(0.082)\n\n\n(0.061)\n\n\n(0.064)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthnic Minority\n\n\n0.557***\n\n\n23.666***\n\n\n-0.386***\n\n\n-0.389***\n\n\n11.010***\n\n\n0.596***\n\n\n0.179***\n\n\n-0.430***\n\n\n0.494***\n\n\n\n\n\n\n(0.001)\n\n\n(0.0003)\n\n\n(0.002)\n\n\n(0.006)\n\n\n(0.0004)\n\n\n(0.001)\n\n\n(0.002)\n\n\n(0.004)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGender\n\n\n-0.617***\n\n\n-0.128**\n\n\n-0.378***\n\n\n-0.030\n\n\n-0.706***\n\n\n-0.507***\n\n\n0.204\n\n\n-0.164\n\n\n-0.182\n\n\n\n\n\n\n(0.185)\n\n\n(0.047)\n\n\n(0.106)\n\n\n(0.204)\n\n\n(0.007)\n\n\n(0.029)\n\n\n(0.119)\n\n\n(0.130)\n\n\n(0.138)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge\n\n\n-0.023***\n\n\n-0.024***\n\n\n-0.001\n\n\n-0.008***\n\n\n-0.005***\n\n\n0.054***\n\n\n-0.010***\n\n\n-0.024***\n\n\n-0.025***\n\n\n\n\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.001)\n\n\n(0.0005)\n\n\n(0.001)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEducation\n\n\n-0.144***\n\n\n-0.035\n\n\n-0.017\n\n\n-0.052\n\n\n-0.083\n\n\n-0.036\n\n\n-0.042\n\n\n-0.103***\n\n\n-0.105***\n\n\n\n\n\n\n(0.033)\n\n\n(0.038)\n\n\n(0.035)\n\n\n(0.034)\n\n\n(0.060)\n\n\n(0.045)\n\n\n(0.036)\n\n\n(0.029)\n\n\n(0.030)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIncome\n\n\n-0.023\n\n\n0.216**\n\n\n0.118\n\n\n-0.067\n\n\n-0.165\n\n\n0.162*\n\n\n-0.050\n\n\n-0.008\n\n\n0.070\n\n\n\n\n\n\n(0.064)\n\n\n(0.081)\n\n\n(0.071)\n\n\n(0.065)\n\n\n(0.115)\n\n\n(0.080)\n\n\n(0.069)\n\n\n(0.057)\n\n\n(0.059)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConstant\n\n\n48.626***\n\n\n-1.783***\n\n\n0.004***\n\n\n17.170***\n\n\n-12.969***\n\n\n-107.485***\n\n\n18.189***\n\n\n51.982***\n\n\n50.057***\n\n\n\n\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0001)\n\n\n(0.0002)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAkaike Inf. Crit.\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n3,975.283\n\n\n\n\n\n\n\n\nNote:\n\n\np&lt;0.05; p&lt;0.01; p&lt;0.001\n\n\n\n\nThe format of the regression table on our Danish model is not ideal since the names of the parties are quite long and overlap. Blame this on my lack of knowledge of abbreviations of Danish parties…",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "href": "session3/session3.html#interpreting-a-multinomial-regression-table",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.6 Interpreting a Multinomial Regression Table",
    "text": "3.6 Interpreting a Multinomial Regression Table\nWe can see that many many things are going on in this regression table. Let us try to analyze our results step by step.\nFirst of all, we can see that we have many variables that are statistically significant (lots of stars yay!). This is always a good sign. Note also that the baseline was the party AFD. You can see this based on the fact that the category AFD which our DV can take on is not given in our table. This means that whenever we see the results where the DV is one of the parties, R has calculated the coefficients based on the logic that the respondent would have voter for either the party in the dependent variable or the party of the baseline, which in our case is that of the AFD. In more mathematical terms these are several single logistic regressions always with regards to the baseline AFD which are then aggregated to a multinomial regression. And to be slightly more mathematical, this means our DV is technically: \\(1 = DV\\) and then \\(0 = AFD\\).\nTherefore, we can interpret the results exactly like we would for a logistic regression. Last week it was about the likelihood of voting abstention, this week it is the likelihood of voting for the CDU/CSU instead of the AFD, or voting for the SPD instead of the AFD, or voting for Die Linke instead of the AFD, and so on. You get the idea hopefully.\nRemember that these are the coefficients of logistic regressions. We cannot interpret them linearily like in OLS. For now, the regression table tells us something about the statistical significance of our predictors and the direction of association: whether or not a statistically significant predictor increases or decreases the likelihood of voting for either or.\n\n3.6.1 The Hypotheses\nAs a reminder, these were my initial (frankly also bad) hypotheses:\n\nH1: Thinking that immigrants enrich a country’s culture decreases the likelihood of voting for PRRPs.\nH2: Having less trust in politicians increases the likelihood of voting for PRRPs than voting for other parties.\n\nI am now interested to see the effect of positivity toward migration and trust in politicians on the vote choice for each party instead of the AFD. What we can see is that a one-unit increase in positive attitudes toward migration (thinking that immigrants culturally enrich the respondents’ country) raises the likelihood for voting for all other parties instead of voting for the AFD. In the case of the first column, in which the vote was either for the CDU/CSU or the AFD, a one unit increase in stances on immigration results in a higher likelihood of voting of voting for the CDU/CSU than the AFD.\nIf we now turn to trust in politicians and this variable’s effect on vote choice for the different German parties, we can see that overall there is a statistically significant a positive association with having more trust in politicians and also voting for other parties than the AFD. In return, this also means that low trust in politicians raises the likelihood of voting for the AFD.\nYou could obviously exponentiate the values that we have here in order to get the odds-ratio. But I have tortured you enough with ORs and predicted probabilities are much more intuitively interpreted. Therefore, we will calculate them in the next section.",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#predicted-probabilities",
    "href": "session3/session3.html#predicted-probabilities",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.7 Predicted Probabilities",
    "text": "3.7 Predicted Probabilities\nYou all hopefully still remember the idea of predicted probabilities which we have already seen last time for a simply logistic regression. You hold all but one predictor variables (IVs) constant at their mean or another logical value. The one predictor which you do not hold constant you let alternate/vary to estimate the predicted probabilities of this specific variable of interest and the different values it can take on (on your dependent variable). The predicted probabilities can be tricky to code manually and we are not going to do this again but we will use a package that can do this for us.\nThe package is called MNLpred and allows us to specify the variable of interest. This packages makes draws from our posterior distribution (hello Bayesian statistics) and simulates our coefficients n-times (we tell it how many times to run the simulation) and then takes the mean value of all of our simulations. This way, we end up more or less with the same predicted probabilities that we have seen last week. These are much more easily interpreted than relative risk ratios (the odds-ratios of multinomial regressions) and can be plotted.\n\nlibrary(MNLpred)\npred1 &lt;- mnl_pred_ova(\n  model = model_de,\n  # specify data source\n  data = ess_clean,\n  # specify predictor of interest\n  x = \"imueclt\",\n  # the steps which should be used for the simulated prediction\n  by = 1,\n  # this would be for replicability, we do not care about it\n  # here\n  seed = \"random\",\n  # number of simulations\n  nsim = 100,\n  # confidence intervals\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe pred1 object now contains the simulated means for each party at each step of our predictor of interest meaning that there are 10 simulated mean values for each value that imueclt can take on for each party:\n\npred1$plotdata |&gt; head()\n\n  imueclt vote_de       mean      lower      upper\n1       0 CDU/CSU 0.24148108 0.17194641 0.33373633\n2       1 CDU/CSU 0.18799950 0.13781243 0.25099852\n3       2 CDU/CSU 0.14176525 0.10922007 0.17941263\n4       3 CDU/CSU 0.10345537 0.08500228 0.12351606\n5       4 CDU/CSU 0.07307269 0.05957871 0.08859076\n6       5 CDU/CSU 0.05000290 0.03944856 0.06112018\n\n\nLet’s simulate the exact same thing for our second hypothesis regarding the trust in politicians:\n\npred2 &lt;- mnl_pred_ova(\n  model = model_de,\n  data = ess_clean,\n  x = \"trstplt\",\n  by = 1,\n  seed = \"random\",\n  nsim = 100,\n  probs = c(0.025, 0.975)\n)\n\nMultiplying values with simulated estimates:\n================================================================================\nApplying link function:\n================================================================================\nDone!\n\n\nThe results, which we have both stored respectively in the objects pred1 and pred2 can be used for a visualization with ggplot().\n\nlibrary(ggplot2)\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  # this gives us the confidence intervals\n  geom_ribbon(alpha = 0.1) +\n  # taking the mean of the values\n  geom_line() +\n  # here we display the predicted probabilities for all parties in one plot\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  # putting the values of the y-axis in percentages\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  # the x-axis follows the 0-10 scale of the predictor\n  scale_x_continuous(breaks = c(0:10)) +\n  # specifying the ggplot theme\n  theme_minimal() +\n  # lastly you only need to label your axes; Always label your axes ;)\n  labs(y = \"Predicted probabilities\",\n       x = \"Trust in Politicians\") \n\n\n\n\n\n\n\nHere we can see very well by how many percent the likelihood increases or decreases for each party given that our independent variable, our predictor, of trust in politicians increases (increasing values mean more trust in politicians).\nWe can also visualize our predicted probabilities in one single plot. I made the effort of coordinating the colors so that they would be displayed in the colors of the parties. If you want to have a color selector to get the HEX color codes, you can click on this link: (it will say Google Farbwähler, which is not a scam but German…). As by recently, R will also display the color you have selected.\n\nggplot(data = pred2$plotdata, aes(\n  x = trstplt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower, ymax = upper), stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Trust in Politicians\") +\n  theme_minimal()\n\n\n\n\n\n\n\nThis here is the plot for our first hypothesis for which we have stored the predicted probabilities in the object pred1:\n\nlibrary(ggplot2)\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  ymin = lower,\n  ymax = upper\n)) +\n  geom_ribbon(alpha = 0.1) + # Confidence intervals\n  geom_line() + # Mean\n  facet_wrap(. ~ vote_de, ncol = 2) +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) + # % labels\n  scale_x_continuous(breaks = c(0:10)) +\n  theme_minimal() +\n  labs(y = \"Predicted probabilities\",\n       x = \"Positivity towards Immigrants\") # Always label your axes ;)\n\n\n\n\n\n\n\nAnd here the code which puts all the predicted probabilities in one plot:\n\nggplot(data = pred1$plotdata, aes(\n  x = imueclt,\n  y = mean,\n  color = as.factor(vote_de)\n)) +\n  geom_smooth(aes(ymin = lower,\n                  ymax = upper),\n              stat = \"identity\") +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +\n  scale_x_continuous(breaks = c(0:10)) +\n  scale_color_manual(\n    values = c(\n      \"#03c2fc\",\n      \"#000000\",\n      \"#f26dd5\",\n      \"#FFFF00\",\n      \"#00e81b\",\n      \"#fa0000\"\n    ),\n    name = \"Vote\",\n    labels = c(\"AFD\", \"CDU\", \"DIE LINKE\", \"FDP\",\n               \"GRUENE\", \"SPD\")\n  ) +\n  ylab(\"Predicted Probability Vote\") +\n  xlab(\"Positivity towards Immigration\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#diagnostics-of-multinomial-models",
    "href": "session3/session3.html#diagnostics-of-multinomial-models",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.8 Diagnostics of Multinomial Models",
    "text": "3.8 Diagnostics of Multinomial Models\nI have talked about diagnostics of models before. This will be the first time that we really touch upon that in models that are not linear like OLS. Usually this is a step which you should take between the building and the final interpretation of your model.\nThe estimates of your model change depending on several influences. The number of predictors, the scaling of your predictors, the scaling of your dependent variable or the coding of your dependent variable. All these kind of things (and many more) will have an effect on your model’s results. We need to be sure that we have a good amount of variables to account for enough variance. But we also need to make sure that we do not overfit our model, meaning that we put in too many predictors for example. We also need to make sure that our model is not biased by the scaling of our variables. This is why we need to check for multicollinearity, heteroskedasticity and other things.\nWe are firstly concerned with the goodness of fit of our model. In a linear model using the OLS method, we have looked at the \\(R^2\\) and adjusted \\(R^2\\) of the models. This tells us something about how much variance of the DV is explained by our IVs. Unfortunately, this measure does not exist for logistic or multinomial models. But the good news is that we can calculate something that is called McFadden’s Pseudo \\(R^2\\). It is interpreted in a similar way as you would do it with a normal \\(R^2\\) meaning that anything ranging between 0.2 and 0.4 is a result that should make us happy.\nThis is how you do this in R:\n\n# you obviously need to install the package first\nlibrary(pscl)\n\nClasses and Methods for R originally developed in the\nPolitical Science Computational Laboratory\nDepartment of Political Science\nStanford University (2002-2015),\nby and under the direction of Simon Jackman.\nhurdle and zeroinfl functions by Achim Zeileis.\n\npR2(model_de)\n\nfitting null model for pseudo-r2\n# weights:  12 (5 variable)\ninitial  value 2512.046776 \niter  10 value 2158.240962\niter  10 value 2158.240953\niter  10 value 2158.240953\nfinal  value 2158.240953 \nconverged\n\n\n          llh       llhNull            G2      McFadden          r2ML \n-1925.8149021 -2158.2409526   464.8521011     0.1076924     0.2821995 \n         r2CU \n    0.2958110 \n\n\n\n3.8.1 Hetereoskedasticity and Multicollinearity\nThen there are issues of scary words like multicollinearity or heteroskedasticity (oftentimes also refered to as “heteroske-something”). These two things describe two phenomena that can skew our estimations and, in the worst case scenario, will lead to wrong inferences. Therefore, we must check for them in all different kinds of models, be it a simple model using the OLS method, or a logistic regression or a multinomial regression. There are ways to test for potential problems that might arise and also ways to work our way around them if ever we encounter them.\n\n3.8.2 Multicollinearity and how to eliminate it\nFor now, we will only look at the potential issue of multicollinearity. It occurs when your independent variables are correlated among each other. This means that they vary very similarly in their values and measure either similar things or measure things the same way. The higher the multicollinearity within your model, the less reliable are your statistical inferences.\nWe can detect the amount and measure of (multi)collinearity by calculating the Variance Inflation Factor (VIF). It measures the amount of correlation between our predictors. The VIF should be below 10. If it is below 0.2, this is a potential problem. Anything below 0.1 should have us really worried. To do this in R, we use the vif() function of the car package. However, it does not work on the object of a multinomial model. Thus, we cheat our way around it and build a GLM model (glm()) in which we set our DV as factors and pretend that they are binomially distributed. This way, R sort of manually calculates the individual logistic regressions according to a baseline and we can calculate the VIF for the IVs individually.\n\nmodel_vif &lt;-\n  glm(\n    as.factor(vote_de) ~ imueclt  + stflife + trstplt + blgetmg +\n      gndr + yrbrn + eduyrs + hinctnta,\n    data = ess_clean,\n    family = binomial()\n  )\n\ncar::vif(model_vif)\n\n imueclt  stflife  trstplt  blgetmg     gndr    yrbrn   eduyrs hinctnta \n1.350982 1.141501 1.274526 1.013258 1.037425 1.112390 1.260009 1.202217 \n\n\nBased on the results, we can see that our variance is not inflated since all values are below 10. That is great news! A VIF of 1 means that there is no correlation within our predictors, a VIF between 1 and 5 (which is quite normal) indicates slight correlation, and a VIF between 5 and 10 shows a strong correlation.\nIf, however, you should encounter issues of multicollinearity, you should test the VIFs for different versions of your model by starting to drop the IV with the highest VIF and see how that affects your VIFs overall. Or you check the variables which have high values, see if theoretically speaking they measure similar things, and combine them into a single measure.\nIt is important to do this step in order to test the validity and reliability of our models!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "href": "session3/session3.html#goodness-of-fits-and-its-other-measures",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.9 Goodness of Fits and its other measures",
    "text": "3.9 Goodness of Fits and its other measures\nYou have seen me use the term goodness of fit before and that this becomes very important in quantitative research when you try to model statistical relationships. Until now, we have always only modeled one model and then interpreted its coefficients and model values. We have seen the \\(R^2\\) and adjusted \\(R^2\\) and we have mostly seen bad OLS models which showed very low values in both these measures. However, this measure does not always exist for generalized linear models. Thus, statisticians have come up with other ways to compare models and their goodness of fit. As a rule of thumb, we should always favor models which explain as much as possible by not making too many (strong) assumptions and overfitting our predictors, e.g. adding too many in one regression etc. In one of your introduction to (political) science classes, you might have heard of Ockham’s razor; this is the same idea but for statistical models.\nGoodness of fit in our case refers to how well the model which we have constructed, fits the set of our made observations. Thus, goodness of fit somewhat measures the discrepancy between our observed and expected values given our model. If we do not have an adjusted \\(R^2\\), we need to use other information criteria to determine which model fits best our data. There is quite an abundance of criteria which come to mind. Some of them are related to specific kinds of statistical models, whereas some are more general. The two which I would like to mention here are the AIC and the BIC.\n\n3.9.1 AIC (Akaike Information Criterion)\nDon’t be like me and think for years AIC was a bad abbreviation of Akaike. It actually stands for Akaike Information Criterion. It is calculated based on the number of predictors of our model and how well it reproduces our data (the likelihood estimation). If you go back to our multinomial regressions above, you can see that the the last line of our table shows the AIC for this model. Individually, this information criterion is meaningless. It becomes important when we compare it to an AIC of a similar model and check which one indicates a better fit.\nWhat would a similar model look like? Well, if we dropped one of our IVs for example, we would alter the model a bit but keep its global structure. In that case, we would generate a second but different AIC. Comparing the AIC then tells us something about which model (meaning which composition of model) indicates a better fit.\nWhat is a better AIC? The lower AIC indicates that the model fits our data better than the model with the higher AIC. This is simply a mathematical measure. Stand-alone values of the AIC do not tell us much. They need to be considered in comparison to other values.\n\n3.9.2 BIC (Bayesian Information Criterion)\nBayesian Statistics 1 are super fascinating and I will include them wherever I can. Luckily, the BIC is very common and is to the AIC what the adjusted \\(R^2\\) is to the \\(R^2\\). This means that it is a “stricter” measure of goodness of fit than the AIC. It is quite similar to the AIC but differs in that it penalizes you for adding “less useful” variables to your model (potentially overfitting or overcomplexifing your model). Thus, similarly to the AIC, we should also favor the lower BIC!",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "href": "session3/session3.html#apis-and-fun-data-visualization-optional",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "\n3.10 APIs and Fun Data Visualization (OPTIONAL!)",
    "text": "3.10 APIs and Fun Data Visualization (OPTIONAL!)\nIn this session’s optional section, I will introduce you to the logic of APIs and how to use them in R. Further, we will use data from Spotify (which we will collect through their API) to make fun data visualizations!\n\n3.10.1 What is an API?\nAPI stands for Application Programming Interface. It is a way for two applications to communicate with each other. This is a very technical way of saying that an API allows us to carefully and gently collect data from a server, a website or a database. APIs are a very common way of collecting data from websites. Most websites use internal APIs to communicate with their own databases 2, whereas some websites also offer public APIs which allow you to collect data from their website.\nThink about APIs like this: You go to a restaurant and you want to eat. You could be very rude, ignore every other customer, ignore every social norm you have ever learned and go to the kitchen to tell the chef what you want. Now, if only one person did this, nothing would happen (even if I were surprised if the chef would take your order). But if now every customer did this, started yelling their order at the chef, he would probably just stop working and throw in the towel, or worse, throw you out of the restaurant. To avoid that, there are waiters and waitresses that usually come to your table, take your order, deliver that to the kitchen and once your dish is ready, you will get it to your table. Transposing this onto APIs, the waiter/waitress is our API. They make sure that we can communicate with the kitchen (the database) without disrupting the work of the chef (the server).\nUsually APIs are win-win situations for us and for them. They can control how much data we collect from them, they can dictate the rules and have traces of what we were doing when and where. And we can collect data without disrupting their work. However, sometimes APIs are not public and we need to find other ways to collect data from websites. This is called webscraping and is a bit more complicated and in the appendix.\nSome APIs are more useful than others. The New York Times’ one is unfortunately quite useless. Spotify’s API is more fun, as we will see in a bit. Twitter used to have one of the best developer’s accesses to their data through a wonderful API until he-who-must-not-be-named destroyed the platform. But also government websites have APIs which make it a bit more regulated to access their data.\n\n3.10.2 Spotify API\nFor the sake of the example, we will use the Spotify API to communicate with the Spotify servers and ask them for data. You usually have to sign up for an API on the respective website. If you want to reproduce my code, you will have to sign up for a Spotify API here. Once you have signed up, you will be able to create a new app. This will give you a client ID and a client secret. These are your credentials to access the API. You will need them to access the API.\n\nneeds('spotifyr',\n      'tidyverse',\n      'plotly',\n      'ggimage',\n      'httpuv',\n      'httr', \n      'usethis')\n\n\n3.10.3 System Enivronments and API keys\nWe will get a bit more technical and speak about good practices in R for a second. When you apply to an API or sign up for an access, you will usually get a sort of key, clientID or something to authenticate yourself whenever you make requests to the API. This way, they know that it is you who is making requests and that you are validated by them. This information is sensitive! You do not share it with anyone, and you never show this to anyone. This is why you need to store it somewhere safe. One of the options we have in R is to save them in our system environment. It is a file in which we can store these things. Honestly, the keys to APIs are really long and complicated and you do not want to type them in every time you want to use them. So, we will store them in our system environment. This is a bit more complicated than just typing them in, but it is a good practice. It is also not the safest way to do it but for simplicity’s sake we will do it like this for now.\n\n\n\n\n\n\nIf you want to follow my code and run it on your end, you will have to first sign up to the Spotify developer’s portal, generate the key and clientID and come back to my script then.\n\n\n\nThis line of code opens up a file called .Renviron in which we can store our keys. If you have never done this before, it will open up a new file. If you have done this before, it will open up the file in which you probably already have things stored.\n\nusethis::edit_r_environ()\n\nOnce you have opened the file, you can add the following lines to it. You will have to replace the XXXX with your own keys. Once you have done that, save the file and restart R! This is necessary for the changes to take effect.\n\nSPOTIFY_CLIENT_ID = \"XXXX\"\nSPOTIFY_CLIENT_SECRET = \"XXXX\"\n# this has to be the same localhost as in indicated in the spotify developer portal\nSPOTIFY_REDIRECT_URI = \"http://localhost:1410/\"\n\nNow that we have added the sensitive information to our R Environment, it will remain there until we decide to delete it. If we need to load it now, we can run these lines of code:\n\nclient_id &lt;- Sys.getenv(\"SPOTIFY_CLIENT_ID\")\nclient_secret &lt;- Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\nredirect_uri &lt;- Sys.getenv(\"SPOTIFY_REDIRECT_URI\")",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  },
  {
    "objectID": "session3/session3.html#footnotes",
    "href": "session3/session3.html#footnotes",
    "title": "\n3  Multinomial Regressions in R\n",
    "section": "",
    "text": "The second school of doing statistics. What we are doing is called frequentist statistics. Bayesian statistics are a bit more complicated and require a different way of thinking about statistics but they are the more intuitive way of conducting statistics (without p-values but with something you could call certainty). They are also more computationally expensive and thus, have only gained traction over the recent decades. They are gaining more and more popularity. If you are interested in learning more about them, I recommend the book by Richard McElreath (2016) Statistical Rethinking: A Bayesian Course with Examples in R and Stan. He also has lectures on YouTube that follow the book. Definitely worth a try!↩︎\nThis is a fun exercise for webscraping and you can imitate the websites’ calls to their server API to collect data but more on that at a later point.↩︎",
    "crumbs": [
      "Multinomial Regression",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multinomial Regressions in R</span>"
    ]
  }
]